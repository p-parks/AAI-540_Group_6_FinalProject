{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# READ dataset\\CEAS_08.csv and make a train and test split then save to new csv files\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('dataset/CEAS_08.csv')\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# make a new dir for autogloun to read the data\n",
    "os.makedirs('dataset_autogluon', exist_ok=True)\n",
    "\n",
    "train.to_csv('dataset_autogluon/train.csv', index=False)\n",
    "test.to_csv('dataset_autogluon/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20241003_000618\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.5\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          32\n",
      "Memory Avail:       47.80 GB / 63.75 GB (75.0%)\n",
      "Disk Space Avail:   3152.20 GB / 3725.17 GB (84.6%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (31323 samples, 70.25 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20241003_000618\"\n",
      "Train Data Rows:    31323\n",
      "Train Data Columns: 6\n",
      "Label Column:       label\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    49001.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.76 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "C:\\Users\\paula\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\common\\features\\infer_types.py:118: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  result = pd.to_datetime(X, errors=\"coerce\", format=\"mixed\")\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['sender', 'subject', 'body']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])                        : 1 | ['urls']\n",
      "\t\t('object', [])                     : 1 | ['receiver']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['date']\n",
      "\t\t('object', ['text'])               : 3 | ['sender', 'subject', 'body']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    1 | ['receiver']\n",
      "\t\t('category', ['text_as_category'])  :    2 | ['sender', 'subject']\n",
      "\t\t('int', ['binned', 'text_special']) :   76 | ['sender.char_count', 'sender.word_count', 'sender.capital_ratio', 'sender.lower_ratio', 'sender.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :    1 | ['urls']\n",
      "\t\t('int', ['datetime_as_int'])        :    5 | ['date', 'date.year', 'date.month', 'date.day', 'date.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 6913 | ['__nlp__.00', '__nlp__.00 00', '__nlp__.000', '__nlp__.000 gorillas', '__nlp__.0000', ...]\n",
      "\t62.8s = Fit runtime\n",
      "\t6 features in original data used to generate 6998 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 416.69 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 64.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.07981355553427194, Train Rows: 28823, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.4716\t = Validation score   (accuracy)\n",
      "\t4.65s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.4712\t = Validation score   (accuracy)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9972\t = Validation score   (accuracy)\n",
      "\t7.52s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9972\t = Validation score   (accuracy)\n",
      "\t5.89s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9972\t = Validation score   (accuracy)\n",
      "\t8.98s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9968\t = Validation score   (accuracy)\n",
      "\t7.38s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "Fitting model: XGBoost ...\n",
      "\t0.9972\t = Validation score   (accuracy)\n",
      "\t12.54s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.1.1`.\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 0.5, 'XGBoost': 0.5}\n",
      "\t0.998\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 143.0s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10961.8 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20241003_000618\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "data_root = 'dataset_autogluon/'\n",
    "train_data = TabularDataset(data_root + 'train.csv')\n",
    "test_data = TabularDataset(data_root + 'test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='label').fit(train_data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictor.predict(test_data)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9948920955178138,\n",
       " 'balanced_accuracy': 0.9950410126196556,\n",
       " 'mcc': 0.9896183049345015,\n",
       " 'roc_auc': 0.9998112745286708,\n",
       " 'f1': 0.995475113122172,\n",
       " 'precision': 0.997054158169046,\n",
       " 'recall': 0.9939010616670432}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.995020</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.794080</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>12.535784</td>\n",
       "      <td>0.794080</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>12.535784</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.994892</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.239892</td>\n",
       "      <td>0.228064</td>\n",
       "      <td>20.095648</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.039007</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.994254</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.443312</td>\n",
       "      <td>0.077550</td>\n",
       "      <td>7.520857</td>\n",
       "      <td>0.443312</td>\n",
       "      <td>0.077550</td>\n",
       "      <td>7.520857</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.994126</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.433464</td>\n",
       "      <td>0.067508</td>\n",
       "      <td>7.378530</td>\n",
       "      <td>0.433464</td>\n",
       "      <td>0.067508</td>\n",
       "      <td>7.378530</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.993871</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.418768</td>\n",
       "      <td>0.068009</td>\n",
       "      <td>8.983627</td>\n",
       "      <td>0.418768</td>\n",
       "      <td>0.068009</td>\n",
       "      <td>8.983627</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.993743</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.393245</td>\n",
       "      <td>0.066508</td>\n",
       "      <td>5.888525</td>\n",
       "      <td>0.393245</td>\n",
       "      <td>0.066508</td>\n",
       "      <td>5.888525</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>0.465969</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>5.245612</td>\n",
       "      <td>0.746475</td>\n",
       "      <td>3.946615</td>\n",
       "      <td>5.245612</td>\n",
       "      <td>0.746475</td>\n",
       "      <td>3.946615</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>0.465969</td>\n",
       "      <td>0.4716</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>6.086363</td>\n",
       "      <td>0.786382</td>\n",
       "      <td>4.651834</td>\n",
       "      <td>6.086363</td>\n",
       "      <td>0.786382</td>\n",
       "      <td>4.651834</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
       "0              XGBoost    0.995020     0.9972    accuracy        0.794080   \n",
       "1  WeightedEnsemble_L2    0.994892     0.9980    accuracy        1.239892   \n",
       "2     RandomForestGini    0.994254     0.9972    accuracy        0.443312   \n",
       "3       ExtraTreesEntr    0.994126     0.9968    accuracy        0.433464   \n",
       "4       ExtraTreesGini    0.993871     0.9972    accuracy        0.418768   \n",
       "5     RandomForestEntr    0.993743     0.9972    accuracy        0.393245   \n",
       "6       KNeighborsDist    0.465969     0.4712    accuracy        5.245612   \n",
       "7       KNeighborsUnif    0.465969     0.4716    accuracy        6.086363   \n",
       "\n",
       "   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.150015  12.535784                 0.794080                0.150015   \n",
       "1       0.228064  20.095648                 0.002500                0.000500   \n",
       "2       0.077550   7.520857                 0.443312                0.077550   \n",
       "3       0.067508   7.378530                 0.433464                0.067508   \n",
       "4       0.068009   8.983627                 0.418768                0.068009   \n",
       "5       0.066508   5.888525                 0.393245                0.066508   \n",
       "6       0.746475   3.946615                 5.245612                0.746475   \n",
       "7       0.786382   4.651834                 6.086363                0.786382   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0          12.535784            1       True          7  \n",
       "1           0.039007            2       True          8  \n",
       "2           7.520857            1       True          3  \n",
       "3           7.378530            1       True          6  \n",
       "4           8.983627            1       True          5  \n",
       "5           5.888525            1       True          4  \n",
       "6           3.946615            1       True          2  \n",
       "7           4.651834            1       True          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: WeightedEnsemble_L2\n"
     ]
    }
   ],
   "source": [
    "best_model = predictor.model_best\n",
    "print(f\"Best model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeightedEnsemble_L2',\n",
       " 'model_type': 'WeightedEnsembleModel',\n",
       " 'problem_type': 'binary',\n",
       " 'eval_metric': 'accuracy',\n",
       " 'stopping_metric': 'accuracy',\n",
       " 'fit_time': 0.03900718688964844,\n",
       " 'num_classes': 2,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.0004999637603759766,\n",
       " 'val_score': 0.998,\n",
       " 'hyperparameters': {'use_orig_features': False,\n",
       "  'max_base_models': 25,\n",
       "  'max_base_models_per_type': 5,\n",
       "  'save_bag_folds': True},\n",
       " 'hyperparameters_fit': {},\n",
       " 'hyperparameters_nondefault': ['save_bag_folds'],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'valid_raw_types': None,\n",
       "  'valid_special_types': None,\n",
       "  'ignored_type_group_special': None,\n",
       "  'ignored_type_group_raw': None,\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None,\n",
       "  'predict_1_batch_size': None,\n",
       "  'temperature_scalar': None,\n",
       "  'drop_unique': False},\n",
       " 'num_features': 2,\n",
       " 'features': ['RandomForestGini', 'XGBoost'],\n",
       " 'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x1854fcfa850>,\n",
       " 'memory_size': 3465,\n",
       " 'compile_time': None,\n",
       " 'is_initialized': True,\n",
       " 'is_fit': True,\n",
       " 'is_valid': True,\n",
       " 'can_infer': True,\n",
       " 'bagged_info': {'child_model_type': 'GreedyWeightedEnsembleModel',\n",
       "  'num_child_models': 1,\n",
       "  'child_model_names': ['S1F1'],\n",
       "  '_n_repeats': 1,\n",
       "  '_k_per_n_repeat': [1],\n",
       "  '_random_state': 2,\n",
       "  'low_memory': False,\n",
       "  'bagged_mode': False,\n",
       "  'max_memory_size': 3465,\n",
       "  'min_memory_size': 3465,\n",
       "  'child_hyperparameters': {'ensemble_size': 25, 'subsample_size': 1000000},\n",
       "  'child_hyperparameters_fit': {'ensemble_size': 2},\n",
       "  'child_ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "   'max_time_limit_ratio': 1.0,\n",
       "   'max_time_limit': None,\n",
       "   'min_time_limit': 0,\n",
       "   'valid_raw_types': None,\n",
       "   'valid_special_types': None,\n",
       "   'ignored_type_group_special': None,\n",
       "   'ignored_type_group_raw': None,\n",
       "   'get_features_kwargs': None,\n",
       "   'get_features_kwargs_extra': None,\n",
       "   'predict_1_batch_size': None,\n",
       "   'temperature_scalar': None,\n",
       "   'drop_unique': False}},\n",
       " 'stacker_info': {'num_base_models': 2,\n",
       "  'base_model_names': ['RandomForestGini', 'XGBoost']},\n",
       " 'children_info': {'S1F1': {'name': 'S1F1',\n",
       "   'model_type': 'GreedyWeightedEnsembleModel',\n",
       "   'problem_type': 'binary',\n",
       "   'eval_metric': 'accuracy',\n",
       "   'stopping_metric': 'accuracy',\n",
       "   'fit_time': 0.03900718688964844,\n",
       "   'num_classes': 2,\n",
       "   'quantile_levels': None,\n",
       "   'predict_time': None,\n",
       "   'val_score': None,\n",
       "   'hyperparameters': {'ensemble_size': 25, 'subsample_size': 1000000},\n",
       "   'hyperparameters_fit': {'ensemble_size': 2},\n",
       "   'hyperparameters_nondefault': [],\n",
       "   'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "    'max_time_limit_ratio': 1.0,\n",
       "    'max_time_limit': None,\n",
       "    'min_time_limit': 0,\n",
       "    'valid_raw_types': None,\n",
       "    'valid_special_types': None,\n",
       "    'ignored_type_group_special': None,\n",
       "    'ignored_type_group_raw': None,\n",
       "    'get_features_kwargs': None,\n",
       "    'get_features_kwargs_extra': None,\n",
       "    'predict_1_batch_size': None,\n",
       "    'temperature_scalar': None,\n",
       "    'drop_unique': False},\n",
       "   'num_features': 2,\n",
       "   'features': ['RandomForestGini', 'XGBoost'],\n",
       "   'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x1854fcfb5d0>,\n",
       "   'memory_size': 5317,\n",
       "   'compile_time': None,\n",
       "   'is_initialized': True,\n",
       "   'is_fit': True,\n",
       "   'is_valid': True,\n",
       "   'can_infer': True,\n",
       "   'model_weights': {'RandomForestGini': 0.5, 'XGBoost': 0.5}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = predictor.info()\n",
    "model_info['model_info'][best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'XGBoost',\n",
       " 'model_type': 'XGBoostModel',\n",
       " 'problem_type': 'binary',\n",
       " 'eval_metric': 'accuracy',\n",
       " 'stopping_metric': 'accuracy',\n",
       " 'fit_time': 12.535783529281616,\n",
       " 'num_classes': 2,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.15001463890075684,\n",
       " 'val_score': 0.9972,\n",
       " 'hyperparameters': {'n_estimators': 10000,\n",
       "  'learning_rate': 0.1,\n",
       "  'n_jobs': -1,\n",
       "  'proc.max_category_levels': 100,\n",
       "  'objective': 'binary:logistic',\n",
       "  'booster': 'gbtree'},\n",
       " 'hyperparameters_fit': {'n_estimators': 174},\n",
       " 'hyperparameters_nondefault': [],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'valid_raw_types': ['bool', 'int', 'float', 'category'],\n",
       "  'valid_special_types': None,\n",
       "  'ignored_type_group_special': None,\n",
       "  'ignored_type_group_raw': None,\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None,\n",
       "  'predict_1_batch_size': None,\n",
       "  'temperature_scalar': None},\n",
       " 'num_features': 6998,\n",
       " 'features': ['urls',\n",
       "  'sender',\n",
       "  'receiver',\n",
       "  'subject',\n",
       "  'date',\n",
       "  'date.year',\n",
       "  'date.month',\n",
       "  'date.day',\n",
       "  'date.dayofweek',\n",
       "  'sender.char_count',\n",
       "  'sender.word_count',\n",
       "  'sender.capital_ratio',\n",
       "  'sender.lower_ratio',\n",
       "  'sender.digit_ratio',\n",
       "  'sender.special_ratio',\n",
       "  'sender.symbol_count.@',\n",
       "  'sender.symbol_ratio.@',\n",
       "  'sender.symbol_count..',\n",
       "  'sender.symbol_ratio..',\n",
       "  'sender.symbol_count. ',\n",
       "  'sender.symbol_ratio. ',\n",
       "  'sender.symbol_count.-',\n",
       "  'sender.symbol_ratio.-',\n",
       "  'subject.char_count',\n",
       "  'subject.word_count',\n",
       "  'subject.capital_ratio',\n",
       "  'subject.lower_ratio',\n",
       "  'subject.digit_ratio',\n",
       "  'subject.special_ratio',\n",
       "  'subject.symbol_count.!',\n",
       "  'subject.symbol_ratio.!',\n",
       "  'subject.symbol_count.?',\n",
       "  'subject.symbol_ratio.?',\n",
       "  'subject.symbol_count.%',\n",
       "  'subject.symbol_ratio.%',\n",
       "  'subject.symbol_count.#',\n",
       "  'subject.symbol_ratio.#',\n",
       "  'subject.symbol_count..',\n",
       "  'subject.symbol_ratio..',\n",
       "  'subject.symbol_count.:',\n",
       "  'subject.symbol_ratio.:',\n",
       "  'subject.symbol_count. ',\n",
       "  'subject.symbol_ratio. ',\n",
       "  'subject.symbol_count./',\n",
       "  'subject.symbol_ratio./',\n",
       "  'subject.symbol_count.-',\n",
       "  'subject.symbol_ratio.-',\n",
       "  'body.char_count',\n",
       "  'body.word_count',\n",
       "  'body.capital_ratio',\n",
       "  'body.lower_ratio',\n",
       "  'body.digit_ratio',\n",
       "  'body.special_ratio',\n",
       "  'body.symbol_count.!',\n",
       "  'body.symbol_ratio.!',\n",
       "  'body.symbol_count.?',\n",
       "  'body.symbol_ratio.?',\n",
       "  'body.symbol_count.@',\n",
       "  'body.symbol_ratio.@',\n",
       "  'body.symbol_count.%',\n",
       "  'body.symbol_ratio.%',\n",
       "  'body.symbol_count.$',\n",
       "  'body.symbol_ratio.$',\n",
       "  'body.symbol_count.*',\n",
       "  'body.symbol_ratio.*',\n",
       "  'body.symbol_count.&',\n",
       "  'body.symbol_ratio.&',\n",
       "  'body.symbol_count.#',\n",
       "  'body.symbol_ratio.#',\n",
       "  'body.symbol_count.^',\n",
       "  'body.symbol_ratio.^',\n",
       "  'body.symbol_count..',\n",
       "  'body.symbol_ratio..',\n",
       "  'body.symbol_count.:',\n",
       "  'body.symbol_ratio.:',\n",
       "  'body.symbol_count. ',\n",
       "  'body.symbol_ratio. ',\n",
       "  'body.symbol_count./',\n",
       "  'body.symbol_ratio./',\n",
       "  'body.symbol_count.;',\n",
       "  'body.symbol_ratio.;',\n",
       "  'body.symbol_count.-',\n",
       "  'body.symbol_ratio.-',\n",
       "  'body.symbol_count.=',\n",
       "  'body.symbol_ratio.=',\n",
       "  '__nlp__.00',\n",
       "  '__nlp__.00 00',\n",
       "  '__nlp__.000',\n",
       "  '__nlp__.000 gorillas',\n",
       "  '__nlp__.0000',\n",
       "  '__nlp__.00pm',\n",
       "  '__nlp__.01',\n",
       "  '__nlp__.01 anthrax',\n",
       "  '__nlp__.01 canada',\n",
       "  '__nlp__.0100',\n",
       "  '__nlp__.0100 from',\n",
       "  '__nlp__.0100 ist',\n",
       "  '__nlp__.02',\n",
       "  '__nlp__.02 14',\n",
       "  '__nlp__.02 26',\n",
       "  '__nlp__.02 inches',\n",
       "  '__nlp__.0200',\n",
       "  '__nlp__.0200 full',\n",
       "  '__nlp__.02494',\n",
       "  '__nlp__.03',\n",
       "  '__nlp__.03 2008',\n",
       "  '__nlp__.0300',\n",
       "  '__nlp__.0300 full',\n",
       "  '__nlp__.04',\n",
       "  '__nlp__.0400',\n",
       "  '__nlp__.0400 full',\n",
       "  '__nlp__.05',\n",
       "  '__nlp__.05 oil',\n",
       "  '__nlp__.05 vo',\n",
       "  '__nlp__.0500',\n",
       "  '__nlp__.0500 from',\n",
       "  '__nlp__.06',\n",
       "  '__nlp__.06 sot',\n",
       "  '__nlp__.0600',\n",
       "  '__nlp__.07',\n",
       "  '__nlp__.07 31',\n",
       "  '__nlp__.0700',\n",
       "  '__nlp__.08',\n",
       "  '__nlp__.08 01',\n",
       "  '__nlp__.08 02',\n",
       "  '__nlp__.08 03',\n",
       "  '__nlp__.08 04',\n",
       "  '__nlp__.08 05',\n",
       "  '__nlp__.08 06',\n",
       "  '__nlp__.08 08',\n",
       "  '__nlp__.0800',\n",
       "  '__nlp__.0800 from',\n",
       "  '__nlp__.0800 pst',\n",
       "  '__nlp__.09',\n",
       "  '__nlp__.10',\n",
       "  '__nlp__.10 10',\n",
       "  '__nlp__.10 11',\n",
       "  '__nlp__.10 2007',\n",
       "  '__nlp__.10 2008',\n",
       "  '__nlp__.10 27',\n",
       "  '__nlp__.10 30',\n",
       "  '__nlp__.10 31',\n",
       "  '__nlp__.10 from',\n",
       "  '__nlp__.10 from cnn',\n",
       "  '__nlp__.10 the',\n",
       "  '__nlp__.100',\n",
       "  '__nlp__.1000',\n",
       "  '__nlp__.101',\n",
       "  '__nlp__.11',\n",
       "  '__nlp__.11 07',\n",
       "  '__nlp__.11 08',\n",
       "  '__nlp__.11 11',\n",
       "  '__nlp__.11 12',\n",
       "  '__nlp__.11 2008',\n",
       "  '__nlp__.117',\n",
       "  '__nlp__.12',\n",
       "  '__nlp__.12 14',\n",
       "  '__nlp__.12 2008',\n",
       "  '__nlp__.123',\n",
       "  '__nlp__.125',\n",
       "  '__nlp__.127',\n",
       "  '__nlp__.127 by',\n",
       "  '__nlp__.127 by radish',\n",
       "  '__nlp__.129',\n",
       "  '__nlp__.13',\n",
       "  '__nlp__.13 2008',\n",
       "  '__nlp__.13 feb',\n",
       "  '__nlp__.1300',\n",
       "  '__nlp__.1300 from',\n",
       "  '__nlp__.14',\n",
       "  '__nlp__.14 15',\n",
       "  '__nlp__.14 2008',\n",
       "  '__nlp__.140',\n",
       "  '__nlp__.140 211',\n",
       "  '__nlp__.15',\n",
       "  '__nlp__.15 16',\n",
       "  '__nlp__.15 2008',\n",
       "  '__nlp__.15 49',\n",
       "  '__nlp__.15 50',\n",
       "  '__nlp__.15 feb',\n",
       "  '__nlp__.15 feb 2008',\n",
       "  '__nlp__.16',\n",
       "  '__nlp__.16 2008',\n",
       "  '__nlp__.168',\n",
       "  '__nlp__.17',\n",
       "  '__nlp__.17 2008',\n",
       "  '__nlp__.18',\n",
       "  '__nlp__.18 2008',\n",
       "  '__nlp__.18 feb',\n",
       "  '__nlp__.19',\n",
       "  '__nlp__.19 2008',\n",
       "  '__nlp__.192',\n",
       "  '__nlp__.192 168',\n",
       "  '__nlp__.193',\n",
       "  '__nlp__.199',\n",
       "  '__nlp__.1st',\n",
       "  '__nlp__.1st of',\n",
       "  '__nlp__.1st of october',\n",
       "  '__nlp__.1st of september',\n",
       "  '__nlp__.20',\n",
       "  '__nlp__.20 2008',\n",
       "  '__nlp__.200',\n",
       "  '__nlp__.2000',\n",
       "  '__nlp__.2001',\n",
       "  '__nlp__.2002',\n",
       "  '__nlp__.2003',\n",
       "  '__nlp__.2004',\n",
       "  '__nlp__.2005',\n",
       "  '__nlp__.2006',\n",
       "  '__nlp__.2007',\n",
       "  '__nlp__.2007 09',\n",
       "  '__nlp__.2007 10',\n",
       "  '__nlp__.2007 11',\n",
       "  '__nlp__.2007 at',\n",
       "  '__nlp__.2008',\n",
       "  '__nlp__.2008 01',\n",
       "  '__nlp__.2008 02',\n",
       "  '__nlp__.2008 03',\n",
       "  '__nlp__.2008 04',\n",
       "  '__nlp__.2008 05',\n",
       "  '__nlp__.2008 06',\n",
       "  '__nlp__.2008 07',\n",
       "  '__nlp__.2008 07 31',\n",
       "  '__nlp__.2008 08',\n",
       "  '__nlp__.2008 08 01',\n",
       "  '__nlp__.2008 08 05',\n",
       "  '__nlp__.2008 08 06',\n",
       "  '__nlp__.2008 09',\n",
       "  '__nlp__.2008 10',\n",
       "  '__nlp__.2008 11',\n",
       "  '__nlp__.2008 12',\n",
       "  '__nlp__.2008 13',\n",
       "  '__nlp__.2008 14',\n",
       "  '__nlp__.2008 15',\n",
       "  '__nlp__.2008 16',\n",
       "  '__nlp__.2008 17',\n",
       "  '__nlp__.2008 18',\n",
       "  '__nlp__.2008 19',\n",
       "  '__nlp__.2008 20',\n",
       "  '__nlp__.2008 21',\n",
       "  '__nlp__.2008 22',\n",
       "  '__nlp__.2008 58',\n",
       "  '__nlp__.2008 and',\n",
       "  '__nlp__.2008 at',\n",
       "  '__nlp__.2008 at 10',\n",
       "  '__nlp__.2008 at 11',\n",
       "  '__nlp__.2008 at 12',\n",
       "  '__nlp__.2008 cable',\n",
       "  '__nlp__.2008 crime',\n",
       "  '__nlp__.2008 living wayoflife',\n",
       "  '__nlp__.2008 message',\n",
       "  '__nlp__.2008 message date',\n",
       "  '__nlp__.2008 message id',\n",
       "  '__nlp__.2008 showbiz',\n",
       "  '__nlp__.2008 showbiz movies',\n",
       "  '__nlp__.2008 tech',\n",
       "  '__nlp__.2008 us',\n",
       "  '__nlp__.2008 us 08',\n",
       "  '__nlp__.2008 world',\n",
       "  '__nlp__.2079970',\n",
       "  '__nlp__.21',\n",
       "  '__nlp__.21 2008',\n",
       "  '__nlp__.211',\n",
       "  '__nlp__.22',\n",
       "  '__nlp__.23',\n",
       "  '__nlp__.235',\n",
       "  '__nlp__.24',\n",
       "  '__nlp__.24 10',\n",
       "  '__nlp__.24 10 27',\n",
       "  '__nlp__.24 10 30',\n",
       "  '__nlp__.24 10 31',\n",
       "  '__nlp__.24 2008',\n",
       "  '__nlp__.24 hours',\n",
       "  '__nlp__.24 hours to',\n",
       "  '__nlp__.25',\n",
       "  '__nlp__.25 2008',\n",
       "  '__nlp__.250',\n",
       "  '__nlp__.2520262',\n",
       "  '__nlp__.255',\n",
       "  '__nlp__.26',\n",
       "  '__nlp__.26 15',\n",
       "  '__nlp__.26 15 49',\n",
       "  '__nlp__.26 15 50',\n",
       "  '__nlp__.26 2008',\n",
       "  '__nlp__.269',\n",
       "  '__nlp__.27',\n",
       "  '__nlp__.27 2008',\n",
       "  '__nlp__.28',\n",
       "  '__nlp__.28 2008',\n",
       "  '__nlp__.29',\n",
       "  '__nlp__.29 2008',\n",
       "  '__nlp__.2gb',\n",
       "  '__nlp__.2nd',\n",
       "  '__nlp__.2to3',\n",
       "  '__nlp__.30',\n",
       "  '__nlp__.300',\n",
       "  '__nlp__.3000',\n",
       "  '__nlp__.3000 40tangomu',\n",
       "  '__nlp__.3000 mailing',\n",
       "  '__nlp__.3000 python',\n",
       "  '__nlp__.3000 python 3000',\n",
       "  '__nlp__.30th',\n",
       "  '__nlp__.30th of',\n",
       "  '__nlp__.30th of october',\n",
       "  '__nlp__.30th of september',\n",
       "  '__nlp__.31',\n",
       "  '__nlp__.32',\n",
       "  '__nlp__.32 bit',\n",
       "  '__nlp__.33',\n",
       "  '__nlp__.34',\n",
       "  '__nlp__.35',\n",
       "  '__nlp__.36',\n",
       "  '__nlp__.37',\n",
       "  '__nlp__.38',\n",
       "  '__nlp__.39',\n",
       "  '__nlp__.3d',\n",
       "  '__nlp__.3rd',\n",
       "  '__nlp__.40',\n",
       "  '__nlp__.40 of',\n",
       "  '__nlp__.4096',\n",
       "  '__nlp__.40cognosys',\n",
       "  '__nlp__.40tangomu',\n",
       "  '__nlp__.41',\n",
       "  '__nlp__.42',\n",
       "  '__nlp__.43',\n",
       "  '__nlp__.44',\n",
       "  '__nlp__.45',\n",
       "  '__nlp__.46',\n",
       "  '__nlp__.47',\n",
       "  '__nlp__.48',\n",
       "  '__nlp__.49',\n",
       "  '__nlp__.4th',\n",
       "  '__nlp__.50',\n",
       "  '__nlp__.500',\n",
       "  '__nlp__.51',\n",
       "  '__nlp__.52',\n",
       "  '__nlp__.53',\n",
       "  '__nlp__.54',\n",
       "  '__nlp__.55',\n",
       "  '__nlp__.550',\n",
       "  '__nlp__.56',\n",
       "  '__nlp__.57',\n",
       "  '__nlp__.58',\n",
       "  '__nlp__.58 pm',\n",
       "  '__nlp__.59',\n",
       "  '__nlp__.60',\n",
       "  '__nlp__.61',\n",
       "  '__nlp__.63',\n",
       "  '__nlp__.64',\n",
       "  '__nlp__.64 bit',\n",
       "  '__nlp__.64bit',\n",
       "  '__nlp__.65',\n",
       "  '__nlp__.66',\n",
       "  '__nlp__.67',\n",
       "  '__nlp__.69',\n",
       "  '__nlp__.70',\n",
       "  '__nlp__.72',\n",
       "  '__nlp__.75',\n",
       "  '__nlp__.78',\n",
       "  '__nlp__.79',\n",
       "  '__nlp__.7bit',\n",
       "  '__nlp__.80',\n",
       "  '__nlp__.800',\n",
       "  '__nlp__.802',\n",
       "  '__nlp__.83',\n",
       "  '__nlp__.85',\n",
       "  '__nlp__.8859',\n",
       "  '__nlp__.8859 format',\n",
       "  '__nlp__.8859 format flowed',\n",
       "  '__nlp__.89',\n",
       "  '__nlp__.90',\n",
       "  '__nlp__.911',\n",
       "  '__nlp__.911 call',\n",
       "  '__nlp__.93',\n",
       "  '__nlp__.94',\n",
       "  '__nlp__.95',\n",
       "  '__nlp__.9552',\n",
       "  '__nlp__.97',\n",
       "  '__nlp__.98',\n",
       "  '__nlp__.99',\n",
       "  '__nlp__.99 http',\n",
       "  '__nlp__.9a',\n",
       "  '__nlp__.__',\n",
       "  '__nlp__.______________________________________________',\n",
       "  '__nlp__._______________________________________________',\n",
       "  '__nlp__._______________________________________________ python',\n",
       "  '__nlp__._______________________________________________ python dev',\n",
       "  '__nlp__._______________________________________________ python win32',\n",
       "  '__nlp__._______________________________________________ wekalist',\n",
       "  '__nlp__.__future__',\n",
       "  '__nlp__.__init__',\n",
       "  '__nlp__.a1',\n",
       "  '__nlp__.aa',\n",
       "  '__nlp__.aaai',\n",
       "  '__nlp__.aaas',\n",
       "  '__nlp__.aaron',\n",
       "  '__nlp__.aaron kulkis',\n",
       "  '__nlp__.ab',\n",
       "  '__nlp__.abc',\n",
       "  '__nlp__.ability',\n",
       "  '__nlp__.ability to',\n",
       "  '__nlp__.able',\n",
       "  '__nlp__.able to',\n",
       "  '__nlp__.about',\n",
       "  '__nlp__.about it',\n",
       "  '__nlp__.about the',\n",
       "  '__nlp__.about this',\n",
       "  '__nlp__.about your',\n",
       "  '__nlp__.about your health',\n",
       "  '__nlp__.above',\n",
       "  '__nlp__.absolutely',\n",
       "  '__nlp__.absolutely guaranteed',\n",
       "  '__nlp__.abstract',\n",
       "  '__nlp__.abstracts',\n",
       "  '__nlp__.absurd',\n",
       "  '__nlp__.abuse',\n",
       "  '__nlp__.ac',\n",
       "  '__nlp__.ac nz',\n",
       "  '__nlp__.ac nz mailman',\n",
       "  '__nlp__.ac uk',\n",
       "  '__nlp__.academic',\n",
       "  '__nlp__.academy',\n",
       "  '__nlp__.accept',\n",
       "  '__nlp__.acceptance',\n",
       "  '__nlp__.accepted',\n",
       "  '__nlp__.accepted papers',\n",
       "  '__nlp__.accepted papers will',\n",
       "  '__nlp__.access',\n",
       "  '__nlp__.access to',\n",
       "  '__nlp__.accessible',\n",
       "  '__nlp__.according',\n",
       "  '__nlp__.according to the',\n",
       "  '__nlp__.account',\n",
       "  '__nlp__.account and',\n",
       "  '__nlp__.account and update',\n",
       "  '__nlp__.accounts',\n",
       "  '__nlp__.achieve',\n",
       "  '__nlp__.achieved',\n",
       "  '__nlp__.achieved the',\n",
       "  '__nlp__.acm',\n",
       "  '__nlp__.acm org',\n",
       "  '__nlp__.acpi',\n",
       "  '__nlp__.across',\n",
       "  '__nlp__.across the',\n",
       "  '__nlp__.act',\n",
       "  '__nlp__.action',\n",
       "  '__nlp__.actions',\n",
       "  '__nlp__.active',\n",
       "  '__nlp__.activities',\n",
       "  '__nlp__.activity',\n",
       "  '__nlp__.actual',\n",
       "  '__nlp__.actually',\n",
       "  '__nlp__.acy',\n",
       "  '__nlp__.ad',\n",
       "  '__nlp__.adam',\n",
       "  '__nlp__.adaptive',\n",
       "  '__nlp__.add',\n",
       "  '__nlp__.add inches',\n",
       "  '__nlp__.add inches to',\n",
       "  '__nlp__.added',\n",
       "  '__nlp__.added no',\n",
       "  '__nlp__.added no submission',\n",
       "  '__nlp__.added no virus',\n",
       "  '__nlp__.added to',\n",
       "  '__nlp__.added trojan',\n",
       "  '__nlp__.adding',\n",
       "  '__nlp__.addition',\n",
       "  '__nlp__.addition to',\n",
       "  '__nlp__.additional',\n",
       "  '__nlp__.additional commands',\n",
       "  '__nlp__.additional comments',\n",
       "  '__nlp__.address',\n",
       "  '__nlp__.address book',\n",
       "  '__nlp__.addressed',\n",
       "  '__nlp__.addresses',\n",
       "  '__nlp__.adds',\n",
       "  '__nlp__.admin',\n",
       "  '__nlp__.administration',\n",
       "  '__nlp__.administrator',\n",
       "  '__nlp__.adobe',\n",
       "  '__nlp__.ads',\n",
       "  '__nlp__.ads from',\n",
       "  '__nlp__.advance',\n",
       "  '__nlp__.advanced',\n",
       "  '__nlp__.advances',\n",
       "  '__nlp__.advantage',\n",
       "  '__nlp__.advantage of',\n",
       "  '__nlp__.advertise',\n",
       "  '__nlp__.advertisement',\n",
       "  '__nlp__.advertising',\n",
       "  '__nlp__.advice',\n",
       "  '__nlp__.adwords',\n",
       "  '__nlp__.aew45',\n",
       "  '__nlp__.affect',\n",
       "  '__nlp__.afraid',\n",
       "  '__nlp__.africa',\n",
       "  '__nlp__.after',\n",
       "  '__nlp__.after an',\n",
       "  '__nlp__.after the',\n",
       "  '__nlp__.after years',\n",
       "  '__nlp__.afternoon',\n",
       "  '__nlp__.again',\n",
       "  '__nlp__.against',\n",
       "  '__nlp__.against the',\n",
       "  '__nlp__.age',\n",
       "  '__nlp__.agent',\n",
       "  '__nlp__.agent systems',\n",
       "  '__nlp__.agents',\n",
       "  '__nlp__.ago',\n",
       "  '__nlp__.agree',\n",
       "  '__nlp__.agree with',\n",
       "  '__nlp__.agreed',\n",
       "  '__nlp__.agreed to',\n",
       "  '__nlp__.agreement',\n",
       "  '__nlp__.ahead',\n",
       "  '__nlp__.ai',\n",
       "  '__nlp__.aid',\n",
       "  '__nlp__.aid you',\n",
       "  '__nlp__.aim',\n",
       "  '__nlp__.aims',\n",
       "  '__nlp__.air',\n",
       "  '__nlp__.airlines',\n",
       "  '__nlp__.airport',\n",
       "  '__nlp__.ajax',\n",
       "  '__nlp__.al',\n",
       "  '__nlp__.albany',\n",
       "  '__nlp__.alert',\n",
       "  '__nlp__.alert alert',\n",
       "  '__nlp__.alert criteria',\n",
       "  '__nlp__.alerts',\n",
       "  '__nlp__.alerts my',\n",
       "  '__nlp__.alex',\n",
       "  '__nlp__.algorithm',\n",
       "  '__nlp__.algorithms',\n",
       "  '__nlp__.alias',\n",
       "  '__nlp__.alias trojan',\n",
       "  '__nlp__.aliases',\n",
       "  '__nlp__.aliasing',\n",
       "  '__nlp__.aliasing pipe',\n",
       "  '__nlp__.all',\n",
       "  '__nlp__.all gains',\n",
       "  '__nlp__.all in',\n",
       "  '__nlp__.all is',\n",
       "  '__nlp__.all of',\n",
       "  '__nlp__.all of the',\n",
       "  '__nlp__.all over',\n",
       "  '__nlp__.all over the',\n",
       "  '__nlp__.all results',\n",
       "  '__nlp__.all rights',\n",
       "  '__nlp__.all that',\n",
       "  '__nlp__.all the',\n",
       "  '__nlp__.all you',\n",
       "  '__nlp__.all your',\n",
       "  '__nlp__.allow',\n",
       "  '__nlp__.allow you',\n",
       "  '__nlp__.allowed',\n",
       "  '__nlp__.allowing',\n",
       "  '__nlp__.allows',\n",
       "  '__nlp__.almost',\n",
       "  '__nlp__.alone',\n",
       "  '__nlp__.along',\n",
       "  '__nlp__.along with',\n",
       "  '__nlp__.alpha',\n",
       "  '__nlp__.alphas',\n",
       "  '__nlp__.already',\n",
       "  '__nlp__.already detected',\n",
       "  '__nlp__.also',\n",
       "  '__nlp__.also be',\n",
       "  '__nlp__.also the',\n",
       "  '__nlp__.also thicker',\n",
       "  '__nlp__.alt',\n",
       "  '__nlp__.alter',\n",
       "  '__nlp__.alternative',\n",
       "  '__nlp__.alternatives',\n",
       "  '__nlp__.although',\n",
       "  '__nlp__.always',\n",
       "  '__nlp__.am',\n",
       "  '__nlp__.am not',\n",
       "  '__nlp__.am trying',\n",
       "  '__nlp__.am using',\n",
       "  '__nlp__.amavis',\n",
       "  '__nlp__.amazing',\n",
       "  '__nlp__.amazing growth',\n",
       "  '__nlp__.amazon',\n",
       "  '__nlp__.amazon com',\n",
       "  '__nlp__.amd',\n",
       "  '__nlp__.amd64',\n",
       "  '__nlp__.america',\n",
       "  '__nlp__.american',\n",
       "  '__nlp__.americas',\n",
       "  '__nlp__.among',\n",
       "  '__nlp__.among the',\n",
       "  '__nlp__.amount',\n",
       "  '__nlp__.amount of',\n",
       "  '__nlp__.amsterdam',\n",
       "  '__nlp__.amy',\n",
       "  '__nlp__.an',\n",
       "  '__nlp__.an american',\n",
       "  '__nlp__.an average',\n",
       "  '__nlp__.an economic',\n",
       "  '__nlp__.an email',\n",
       "  '__nlp__.an error',\n",
       "  '__nlp__.an example',\n",
       "  '__nlp__.an html',\n",
       "  '__nlp__.an increase',\n",
       "  '__nlp__.an increase in',\n",
       "  '__nlp__.an incredible',\n",
       "  '__nlp__.an issue',\n",
       "  '__nlp__.an official',\n",
       "  '__nlp__.an open',\n",
       "  '__nlp__.analysis',\n",
       "  '__nlp__.analysis of',\n",
       "  '__nlp__.analyst',\n",
       "  '__nlp__.and',\n",
       "  '__nlp__.and achieve',\n",
       "  '__nlp__.and all',\n",
       "  '__nlp__.and allow',\n",
       "  '__nlp__.and also',\n",
       "  '__nlp__.and am',\n",
       "  '__nlp__.and an',\n",
       "  '__nlp__.and any',\n",
       "  '__nlp__.and applications',\n",
       "  '__nlp__.and are',\n",
       "  '__nlp__.and are very',\n",
       "  '__nlp__.and as',\n",
       "  '__nlp__.and be',\n",
       "  '__nlp__.and can',\n",
       "  '__nlp__.and change',\n",
       "  '__nlp__.and change your',\n",
       "  '__nlp__.and come',\n",
       "  '__nlp__.and data',\n",
       "  '__nlp__.and do',\n",
       "  '__nlp__.and don',\n",
       "  '__nlp__.and even',\n",
       "  '__nlp__.and events',\n",
       "  '__nlp__.and for',\n",
       "  '__nlp__.and get',\n",
       "  '__nlp__.and girth',\n",
       "  '__nlp__.and harmony',\n",
       "  '__nlp__.and has',\n",
       "  '__nlp__.and have',\n",
       "  '__nlp__.and he',\n",
       "  '__nlp__.and how',\n",
       "  '__nlp__.and if',\n",
       "  '__nlp__.and in',\n",
       "  '__nlp__.and information',\n",
       "  '__nlp__.and is',\n",
       "  '__nlp__.and it',\n",
       "  '__nlp__.and its',\n",
       "  '__nlp__.and knowledge',\n",
       "  '__nlp__.and learning',\n",
       "  '__nlp__.and length',\n",
       "  '__nlp__.and make',\n",
       "  '__nlp__.and may',\n",
       "  '__nlp__.and more',\n",
       "  '__nlp__.and my',\n",
       "  '__nlp__.and not',\n",
       "  '__nlp__.and now',\n",
       "  '__nlp__.and on',\n",
       "  '__nlp__.and one',\n",
       "  '__nlp__.and only',\n",
       "  '__nlp__.and or',\n",
       "  '__nlp__.and other',\n",
       "  '__nlp__.and provide',\n",
       "  '__nlp__.and save',\n",
       "  '__nlp__.and see',\n",
       "  '__nlp__.and she',\n",
       "  '__nlp__.and she comes',\n",
       "  '__nlp__.and should',\n",
       "  '__nlp__.and so',\n",
       "  '__nlp__.and some',\n",
       "  '__nlp__.and technology',\n",
       "  '__nlp__.and that',\n",
       "  '__nlp__.and the',\n",
       "  '__nlp__.and their',\n",
       "  '__nlp__.and then',\n",
       "  '__nlp__.and there',\n",
       "  '__nlp__.and they',\n",
       "  '__nlp__.and think',\n",
       "  '__nlp__.and think about',\n",
       "  '__nlp__.and this',\n",
       "  '__nlp__.and to',\n",
       "  '__nlp__.and unfortunately',\n",
       "  '__nlp__.and update',\n",
       "  '__nlp__.and update your',\n",
       "  '__nlp__.and use',\n",
       "  '__nlp__.and was',\n",
       "  '__nlp__.and watch',\n",
       "  '__nlp__.and we',\n",
       "  '__nlp__.and we are',\n",
       "  '__nlp__.and what',\n",
       "  '__nlp__.and when',\n",
       "  '__nlp__.and will',\n",
       "  '__nlp__.and with',\n",
       "  '__nlp__.and would',\n",
       "  '__nlp__.and you',\n",
       "  '__nlp__.and you can',\n",
       "  '__nlp__.and your',\n",
       "  '__nlp__.anders',\n",
       "  '__nlp__.andreas',\n",
       "  '__nlp__.andrew',\n",
       "  '__nlp__.andy',\n",
       "  '__nlp__.angeles',\n",
       "  '__nlp__.ann',\n",
       "  '__nlp__.announce',\n",
       "  '__nlp__.announced',\n",
       "  '__nlp__.announcement',\n",
       "  '__nlp__.announcements',\n",
       "  '__nlp__.annual',\n",
       "  '__nlp__.anonymous',\n",
       "  '__nlp__.another',\n",
       "  '__nlp__.answer',\n",
       "  '__nlp__.answerdry',\n",
       "  '__nlp__.answers',\n",
       "  '__nlp__.anthony',\n",
       "  '__nlp__.anthrax',\n",
       "  '__nlp__.anthrax suspect',\n",
       "  '__nlp__.anti',\n",
       "  '__nlp__.any',\n",
       "  '__nlp__.any of',\n",
       "  '__nlp__.any of the',\n",
       "  '__nlp__.any other',\n",
       "  '__nlp__.any way',\n",
       "  '__nlp__.anybody',\n",
       "  '__nlp__.anymore',\n",
       "  '__nlp__.anyone',\n",
       "  '__nlp__.anyone else',\n",
       "  '__nlp__.anyone know',\n",
       "  '__nlp__.anything',\n",
       "  '__nlp__.anyway',\n",
       "  '__nlp__.anywhere',\n",
       "  '__nlp__.aol',\n",
       "  '__nlp__.aol com',\n",
       "  '__nlp__.ap',\n",
       "  '__nlp__.ap index',\n",
       "  '__nlp__.apache',\n",
       "  '__nlp__.apache org',\n",
       "  '__nlp__.apache org bug',\n",
       "  '__nlp__.apache org by',\n",
       "  '__nlp__.apache org helo',\n",
       "  '__nlp__.apache org spamassassin',\n",
       "  '__nlp__.api',\n",
       "  '__nlp__.apologies',\n",
       "  '__nlp__.apologies for',\n",
       "  '__nlp__.app',\n",
       "  '__nlp__.apparent',\n",
       "  '__nlp__.apparently',\n",
       "  '__nlp__.appeal',\n",
       "  '__nlp__.appear',\n",
       "  '__nlp__.appear in',\n",
       "  '__nlp__.appears',\n",
       "  '__nlp__.appears to',\n",
       "  '__nlp__.apple',\n",
       "  '__nlp__.apple com',\n",
       "  '__nlp__.applicable',\n",
       "  '__nlp__.application',\n",
       "  '__nlp__.applications',\n",
       "  '__nlp__.applications and',\n",
       "  '__nlp__.applications of',\n",
       "  '__nlp__.applied',\n",
       "  '__nlp__.applied to',\n",
       "  '__nlp__.applies',\n",
       "  '__nlp__.apply',\n",
       "  '__nlp__.applying',\n",
       "  '__nlp__.appreciate',\n",
       "  '__nlp__.appreciated',\n",
       "  '__nlp__.approach',\n",
       "  '__nlp__.approach to',\n",
       "  '__nlp__.approaches',\n",
       "  '__nlp__.appropriate',\n",
       "  '__nlp__.approved',\n",
       "  '__nlp__.apps',\n",
       "  '__nlp__.apr',\n",
       "  '__nlp__.apr 2008',\n",
       "  '__nlp__.apr 2008 at',\n",
       "  '__nlp__.apr 26',\n",
       "  '__nlp__.april',\n",
       "  '__nlp__.april 2008',\n",
       "  '__nlp__.ar',\n",
       "  '__nlp__.aran',\n",
       "  '__nlp__.architect',\n",
       "  '__nlp__.architecture',\n",
       "  '__nlp__.architectures',\n",
       "  '__nlp__.archive',\n",
       "  '__nlp__.archives',\n",
       "  '__nlp__.are',\n",
       "  '__nlp__.are 100',\n",
       "  '__nlp__.are all',\n",
       "  '__nlp__.are also',\n",
       "  '__nlp__.are available',\n",
       "  '__nlp__.are being',\n",
       "  '__nlp__.are currently',\n",
       "  '__nlp__.are glad',\n",
       "  '__nlp__.are guaranteed',\n",
       "  '__nlp__.are in',\n",
       "  '__nlp__.are invited',\n",
       "  '__nlp__.are invited to',\n",
       "  '__nlp__.are just',\n",
       "  '__nlp__.are looking',\n",
       "  '__nlp__.are looking for',\n",
       "  '__nlp__.are no',\n",
       "  '__nlp__.are not',\n",
       "  '__nlp__.are not limited',\n",
       "  '__nlp__.are now',\n",
       "  '__nlp__.are receiving',\n",
       "  '__nlp__.are receiving this',\n",
       "  '__nlp__.are still',\n",
       "  '__nlp__.are subscribed',\n",
       "  '__nlp__.are the',\n",
       "  '__nlp__.are the assignee',\n",
       "  '__nlp__.are to',\n",
       "  '__nlp__.are trying',\n",
       "  '__nlp__.are using',\n",
       "  '__nlp__.are very',\n",
       "  '__nlp__.are watching',\n",
       "  '__nlp__.are you',\n",
       "  '__nlp__.area',\n",
       "  '__nlp__.areas',\n",
       "  '__nlp__.areas of',\n",
       "  '__nlp__.aren',\n",
       "  '__nlp__.args',\n",
       "  '__nlp__.argument',\n",
       "  '__nlp__.argumentation',\n",
       "  '__nlp__.arguments',\n",
       "  '__nlp__.arm',\n",
       "  '__nlp__.around',\n",
       "  '__nlp__.around the',\n",
       "  '__nlp__.around the world',\n",
       "  '__nlp__.array',\n",
       "  '__nlp__.arrest',\n",
       "  '__nlp__.arrested',\n",
       "  '__nlp__.art',\n",
       "  '__nlp__.article',\n",
       "  '__nlp__.article pl',\n",
       "  '__nlp__.articles',\n",
       "  '__nlp__.artificial',\n",
       "  '__nlp__.artificial intelligence',\n",
       "  '__nlp__.as',\n",
       "  '__nlp__.as an',\n",
       "  '__nlp__.as by',\n",
       "  '__nlp__.as far',\n",
       "  '__nlp__.as far as',\n",
       "  '__nlp__.as fast',\n",
       "  '__nlp__.as follows',\n",
       "  '__nlp__.as in',\n",
       "  '__nlp__.as in submission',\n",
       "  '__nlp__.as it',\n",
       "  '__nlp__.as long',\n",
       "  '__nlp__.as long as',\n",
       "  '__nlp__.as mouse',\n",
       "  '__nlp__.as much',\n",
       "  '__nlp__.as of',\n",
       "  '__nlp__.as part',\n",
       "  '__nlp__.as possible',\n",
       "  '__nlp__.as result',\n",
       "  '__nlp__.as result of',\n",
       "  '__nlp__.as soon',\n",
       "  '__nlp__.as the',\n",
       "  '__nlp__.as they',\n",
       "  '__nlp__.as to',\n",
       "  '__nlp__.as trojan',\n",
       "  '__nlp__.as trojan agent',\n",
       "  '__nlp__.as trojan downloader',\n",
       "  '__nlp__.as trojan spy',\n",
       "  '__nlp__.as we',\n",
       "  '__nlp__.as well',\n",
       "  '__nlp__.as well as',\n",
       "  '__nlp__.as you',\n",
       "  '__nlp__.ascii',\n",
       "  '__nlp__.ask',\n",
       "  '__nlp__.asked',\n",
       "  '__nlp__.asked to',\n",
       "  '__nlp__.asking',\n",
       "  '__nlp__.asks',\n",
       "  '__nlp__.asp',\n",
       "  '__nlp__.aspects',\n",
       "  '__nlp__.aspects of',\n",
       "  '__nlp__.aspx',\n",
       "  '__nlp__.assertion',\n",
       "  '__nlp__.assigned',\n",
       "  '__nlp__.assignee',\n",
       "  '__nlp__.assignee for',\n",
       "  '__nlp__.assist',\n",
       "  '__nlp__.assist you',\n",
       "  '__nlp__.assistant',\n",
       "  '__nlp__.associate',\n",
       "  '__nlp__.associated',\n",
       "  '__nlp__.associated with',\n",
       "  '__nlp__.association',\n",
       "  '__nlp__.assume',\n",
       "  '__nlp__.assuming',\n",
       "  '__nlp__.astrology',\n",
       "  '__nlp__.astrology com',\n",
       "  '__nlp__.at',\n",
       "  '__nlp__.at 10',\n",
       "  '__nlp__.at 11',\n",
       "  '__nlp__.at 12',\n",
       "  '__nlp__.at all',\n",
       "  '__nlp__.at home',\n",
       "  '__nlp__.at http',\n",
       "  '__nlp__.at http groups',\n",
       "  '__nlp__.at http twitter',\n",
       "  '__nlp__.at http www',\n",
       "  '__nlp__.at http yro',\n",
       "  '__nlp__.at least',\n",
       "  '__nlp__.at least one',\n",
       "  '__nlp__.at nabble',\n",
       "  '__nlp__.at replica',\n",
       "  '__nlp__.at replica classics',\n",
       "  '__nlp__.at some',\n",
       "  '__nlp__.at that',\n",
       "  '__nlp__.at the',\n",
       "  '__nlp__.at the end',\n",
       "  '__nlp__.at the moment',\n",
       "  '__nlp__.at the same',\n",
       "  '__nlp__.at this',\n",
       "  '__nlp__.at work',\n",
       "  '__nlp__.at work and',\n",
       "  '__nlp__.at your',\n",
       "  '__nlp__.atches',\n",
       "  '__nlp__.atlanta',\n",
       "  '__nlp__.atlanta georgia',\n",
       "  '__nlp__.attached',\n",
       "  '__nlp__.attachment',\n",
       "  '__nlp__.attachment was',\n",
       "  '__nlp__.attachments',\n",
       "  '__nlp__.attack',\n",
       "  '__nlp__.attacks',\n",
       "  '__nlp__.attempt',\n",
       "  '__nlp__.attempt to',\n",
       "  '__nlp__.attend',\n",
       "  '__nlp__.attention',\n",
       "  '__nlp__.attribute',\n",
       "  '__nlp__.attributes',\n",
       "  '__nlp__.au',\n",
       "  '__nlp__.auckland',\n",
       "  '__nlp__.audience',\n",
       "  '__nlp__.audio',\n",
       "  '__nlp__.aug',\n",
       "  '__nlp__.aug 2008',\n",
       "  '__nlp__.aug 2008 05',\n",
       "  '__nlp__.aug 2008 11',\n",
       "  '__nlp__.august',\n",
       "  '__nlp__.australia',\n",
       "  '__nlp__.austria',\n",
       "  '__nlp__.auth',\n",
       "  '__nlp__.auth msa',\n",
       "  '__nlp__.auth msa ip',\n",
       "  '__nlp__.authentication',\n",
       "  '__nlp__.author',\n",
       "  '__nlp__.authors',\n",
       "  '__nlp__.authors are',\n",
       "  '__nlp__.auto',\n",
       "  '__nlp__.autolearn',\n",
       "  '__nlp__.automated',\n",
       "  '__nlp__.automatic',\n",
       "  '__nlp__.automatically',\n",
       "  '__nlp__.availability',\n",
       "  '__nlp__.available',\n",
       "  '__nlp__.available at',\n",
       "  '__nlp__.available at http',\n",
       "  '__nlp__.available for',\n",
       "  '__nlp__.available in',\n",
       "  '__nlp__.available on',\n",
       "  '__nlp__.available on the',\n",
       "  '__nlp__.available to',\n",
       "  '__nlp__.avenue',\n",
       "  '__nlp__.average',\n",
       "  '__nlp__.avg',\n",
       "  '__nlp__.avg free',\n",
       "  '__nlp__.avoid',\n",
       "  '__nlp__.award',\n",
       "  '__nlp__.awards',\n",
       "  '__nlp__.aware',\n",
       "  '__nlp__.aware of',\n",
       "  '__nlp__.away',\n",
       "  '__nlp__.awl',\n",
       "  '__nlp__.back',\n",
       "  '__nlp__.back at',\n",
       "  '__nlp__.back to',\n",
       "  '__nlp__.backdoor',\n",
       "  '__nlp__.backend',\n",
       "  '__nlp__.background',\n",
       "  '__nlp__.backup',\n",
       "  '__nlp__.bad',\n",
       "  '__nlp__.bag',\n",
       "  '__nlp__.bank',\n",
       "  '__nlp__.bar',\n",
       "  '__nlp__.barry',\n",
       "  '__nlp__.barry warsaw',\n",
       "  '__nlp__.base',\n",
       "  '__nlp__.based',\n",
       "  '__nlp__.based on',\n",
       "  '__nlp__.based on the',\n",
       "  '__nlp__.bash',\n",
       "  '__nlp__.basic',\n",
       "  '__nlp__.basically',\n",
       "  '__nlp__.basis',\n",
       "  '__nlp__.bat',\n",
       "  '__nlp__.battle',\n",
       "  ...],\n",
       " 'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x18550976fd0>,\n",
       " 'memory_size': 1154543,\n",
       " 'compile_time': None,\n",
       " 'is_initialized': True,\n",
       " 'is_fit': True,\n",
       " 'is_valid': True,\n",
       " 'can_infer': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info['model_info']['XGBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'RandomForestGini',\n",
       " 'model_type': 'RFModel',\n",
       " 'problem_type': 'binary',\n",
       " 'eval_metric': 'accuracy',\n",
       " 'stopping_metric': 'accuracy',\n",
       " 'fit_time': 7.520857095718384,\n",
       " 'num_classes': 2,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.07754969596862793,\n",
       " 'val_score': 0.9972,\n",
       " 'hyperparameters': {'n_estimators': 300,\n",
       "  'max_leaf_nodes': 15000,\n",
       "  'n_jobs': -1,\n",
       "  'random_state': 0,\n",
       "  'bootstrap': True,\n",
       "  'criterion': 'gini'},\n",
       " 'hyperparameters_fit': {'n_estimators': 300},\n",
       " 'hyperparameters_nondefault': ['criterion'],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'valid_raw_types': ['bool', 'int', 'float', 'category'],\n",
       "  'valid_special_types': None,\n",
       "  'ignored_type_group_special': None,\n",
       "  'ignored_type_group_raw': None,\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None,\n",
       "  'predict_1_batch_size': None,\n",
       "  'temperature_scalar': None},\n",
       " 'num_features': 6998,\n",
       " 'features': ['urls',\n",
       "  'sender',\n",
       "  'receiver',\n",
       "  'subject',\n",
       "  'date',\n",
       "  'date.year',\n",
       "  'date.month',\n",
       "  'date.day',\n",
       "  'date.dayofweek',\n",
       "  'sender.char_count',\n",
       "  'sender.word_count',\n",
       "  'sender.capital_ratio',\n",
       "  'sender.lower_ratio',\n",
       "  'sender.digit_ratio',\n",
       "  'sender.special_ratio',\n",
       "  'sender.symbol_count.@',\n",
       "  'sender.symbol_ratio.@',\n",
       "  'sender.symbol_count..',\n",
       "  'sender.symbol_ratio..',\n",
       "  'sender.symbol_count. ',\n",
       "  'sender.symbol_ratio. ',\n",
       "  'sender.symbol_count.-',\n",
       "  'sender.symbol_ratio.-',\n",
       "  'subject.char_count',\n",
       "  'subject.word_count',\n",
       "  'subject.capital_ratio',\n",
       "  'subject.lower_ratio',\n",
       "  'subject.digit_ratio',\n",
       "  'subject.special_ratio',\n",
       "  'subject.symbol_count.!',\n",
       "  'subject.symbol_ratio.!',\n",
       "  'subject.symbol_count.?',\n",
       "  'subject.symbol_ratio.?',\n",
       "  'subject.symbol_count.%',\n",
       "  'subject.symbol_ratio.%',\n",
       "  'subject.symbol_count.#',\n",
       "  'subject.symbol_ratio.#',\n",
       "  'subject.symbol_count..',\n",
       "  'subject.symbol_ratio..',\n",
       "  'subject.symbol_count.:',\n",
       "  'subject.symbol_ratio.:',\n",
       "  'subject.symbol_count. ',\n",
       "  'subject.symbol_ratio. ',\n",
       "  'subject.symbol_count./',\n",
       "  'subject.symbol_ratio./',\n",
       "  'subject.symbol_count.-',\n",
       "  'subject.symbol_ratio.-',\n",
       "  'body.char_count',\n",
       "  'body.word_count',\n",
       "  'body.capital_ratio',\n",
       "  'body.lower_ratio',\n",
       "  'body.digit_ratio',\n",
       "  'body.special_ratio',\n",
       "  'body.symbol_count.!',\n",
       "  'body.symbol_ratio.!',\n",
       "  'body.symbol_count.?',\n",
       "  'body.symbol_ratio.?',\n",
       "  'body.symbol_count.@',\n",
       "  'body.symbol_ratio.@',\n",
       "  'body.symbol_count.%',\n",
       "  'body.symbol_ratio.%',\n",
       "  'body.symbol_count.$',\n",
       "  'body.symbol_ratio.$',\n",
       "  'body.symbol_count.*',\n",
       "  'body.symbol_ratio.*',\n",
       "  'body.symbol_count.&',\n",
       "  'body.symbol_ratio.&',\n",
       "  'body.symbol_count.#',\n",
       "  'body.symbol_ratio.#',\n",
       "  'body.symbol_count.^',\n",
       "  'body.symbol_ratio.^',\n",
       "  'body.symbol_count..',\n",
       "  'body.symbol_ratio..',\n",
       "  'body.symbol_count.:',\n",
       "  'body.symbol_ratio.:',\n",
       "  'body.symbol_count. ',\n",
       "  'body.symbol_ratio. ',\n",
       "  'body.symbol_count./',\n",
       "  'body.symbol_ratio./',\n",
       "  'body.symbol_count.;',\n",
       "  'body.symbol_ratio.;',\n",
       "  'body.symbol_count.-',\n",
       "  'body.symbol_ratio.-',\n",
       "  'body.symbol_count.=',\n",
       "  'body.symbol_ratio.=',\n",
       "  '__nlp__.00',\n",
       "  '__nlp__.00 00',\n",
       "  '__nlp__.000',\n",
       "  '__nlp__.000 gorillas',\n",
       "  '__nlp__.0000',\n",
       "  '__nlp__.00pm',\n",
       "  '__nlp__.01',\n",
       "  '__nlp__.01 anthrax',\n",
       "  '__nlp__.01 canada',\n",
       "  '__nlp__.0100',\n",
       "  '__nlp__.0100 from',\n",
       "  '__nlp__.0100 ist',\n",
       "  '__nlp__.02',\n",
       "  '__nlp__.02 14',\n",
       "  '__nlp__.02 26',\n",
       "  '__nlp__.02 inches',\n",
       "  '__nlp__.0200',\n",
       "  '__nlp__.0200 full',\n",
       "  '__nlp__.02494',\n",
       "  '__nlp__.03',\n",
       "  '__nlp__.03 2008',\n",
       "  '__nlp__.0300',\n",
       "  '__nlp__.0300 full',\n",
       "  '__nlp__.04',\n",
       "  '__nlp__.0400',\n",
       "  '__nlp__.0400 full',\n",
       "  '__nlp__.05',\n",
       "  '__nlp__.05 oil',\n",
       "  '__nlp__.05 vo',\n",
       "  '__nlp__.0500',\n",
       "  '__nlp__.0500 from',\n",
       "  '__nlp__.06',\n",
       "  '__nlp__.06 sot',\n",
       "  '__nlp__.0600',\n",
       "  '__nlp__.07',\n",
       "  '__nlp__.07 31',\n",
       "  '__nlp__.0700',\n",
       "  '__nlp__.08',\n",
       "  '__nlp__.08 01',\n",
       "  '__nlp__.08 02',\n",
       "  '__nlp__.08 03',\n",
       "  '__nlp__.08 04',\n",
       "  '__nlp__.08 05',\n",
       "  '__nlp__.08 06',\n",
       "  '__nlp__.08 08',\n",
       "  '__nlp__.0800',\n",
       "  '__nlp__.0800 from',\n",
       "  '__nlp__.0800 pst',\n",
       "  '__nlp__.09',\n",
       "  '__nlp__.10',\n",
       "  '__nlp__.10 10',\n",
       "  '__nlp__.10 11',\n",
       "  '__nlp__.10 2007',\n",
       "  '__nlp__.10 2008',\n",
       "  '__nlp__.10 27',\n",
       "  '__nlp__.10 30',\n",
       "  '__nlp__.10 31',\n",
       "  '__nlp__.10 from',\n",
       "  '__nlp__.10 from cnn',\n",
       "  '__nlp__.10 the',\n",
       "  '__nlp__.100',\n",
       "  '__nlp__.1000',\n",
       "  '__nlp__.101',\n",
       "  '__nlp__.11',\n",
       "  '__nlp__.11 07',\n",
       "  '__nlp__.11 08',\n",
       "  '__nlp__.11 11',\n",
       "  '__nlp__.11 12',\n",
       "  '__nlp__.11 2008',\n",
       "  '__nlp__.117',\n",
       "  '__nlp__.12',\n",
       "  '__nlp__.12 14',\n",
       "  '__nlp__.12 2008',\n",
       "  '__nlp__.123',\n",
       "  '__nlp__.125',\n",
       "  '__nlp__.127',\n",
       "  '__nlp__.127 by',\n",
       "  '__nlp__.127 by radish',\n",
       "  '__nlp__.129',\n",
       "  '__nlp__.13',\n",
       "  '__nlp__.13 2008',\n",
       "  '__nlp__.13 feb',\n",
       "  '__nlp__.1300',\n",
       "  '__nlp__.1300 from',\n",
       "  '__nlp__.14',\n",
       "  '__nlp__.14 15',\n",
       "  '__nlp__.14 2008',\n",
       "  '__nlp__.140',\n",
       "  '__nlp__.140 211',\n",
       "  '__nlp__.15',\n",
       "  '__nlp__.15 16',\n",
       "  '__nlp__.15 2008',\n",
       "  '__nlp__.15 49',\n",
       "  '__nlp__.15 50',\n",
       "  '__nlp__.15 feb',\n",
       "  '__nlp__.15 feb 2008',\n",
       "  '__nlp__.16',\n",
       "  '__nlp__.16 2008',\n",
       "  '__nlp__.168',\n",
       "  '__nlp__.17',\n",
       "  '__nlp__.17 2008',\n",
       "  '__nlp__.18',\n",
       "  '__nlp__.18 2008',\n",
       "  '__nlp__.18 feb',\n",
       "  '__nlp__.19',\n",
       "  '__nlp__.19 2008',\n",
       "  '__nlp__.192',\n",
       "  '__nlp__.192 168',\n",
       "  '__nlp__.193',\n",
       "  '__nlp__.199',\n",
       "  '__nlp__.1st',\n",
       "  '__nlp__.1st of',\n",
       "  '__nlp__.1st of october',\n",
       "  '__nlp__.1st of september',\n",
       "  '__nlp__.20',\n",
       "  '__nlp__.20 2008',\n",
       "  '__nlp__.200',\n",
       "  '__nlp__.2000',\n",
       "  '__nlp__.2001',\n",
       "  '__nlp__.2002',\n",
       "  '__nlp__.2003',\n",
       "  '__nlp__.2004',\n",
       "  '__nlp__.2005',\n",
       "  '__nlp__.2006',\n",
       "  '__nlp__.2007',\n",
       "  '__nlp__.2007 09',\n",
       "  '__nlp__.2007 10',\n",
       "  '__nlp__.2007 11',\n",
       "  '__nlp__.2007 at',\n",
       "  '__nlp__.2008',\n",
       "  '__nlp__.2008 01',\n",
       "  '__nlp__.2008 02',\n",
       "  '__nlp__.2008 03',\n",
       "  '__nlp__.2008 04',\n",
       "  '__nlp__.2008 05',\n",
       "  '__nlp__.2008 06',\n",
       "  '__nlp__.2008 07',\n",
       "  '__nlp__.2008 07 31',\n",
       "  '__nlp__.2008 08',\n",
       "  '__nlp__.2008 08 01',\n",
       "  '__nlp__.2008 08 05',\n",
       "  '__nlp__.2008 08 06',\n",
       "  '__nlp__.2008 09',\n",
       "  '__nlp__.2008 10',\n",
       "  '__nlp__.2008 11',\n",
       "  '__nlp__.2008 12',\n",
       "  '__nlp__.2008 13',\n",
       "  '__nlp__.2008 14',\n",
       "  '__nlp__.2008 15',\n",
       "  '__nlp__.2008 16',\n",
       "  '__nlp__.2008 17',\n",
       "  '__nlp__.2008 18',\n",
       "  '__nlp__.2008 19',\n",
       "  '__nlp__.2008 20',\n",
       "  '__nlp__.2008 21',\n",
       "  '__nlp__.2008 22',\n",
       "  '__nlp__.2008 58',\n",
       "  '__nlp__.2008 and',\n",
       "  '__nlp__.2008 at',\n",
       "  '__nlp__.2008 at 10',\n",
       "  '__nlp__.2008 at 11',\n",
       "  '__nlp__.2008 at 12',\n",
       "  '__nlp__.2008 cable',\n",
       "  '__nlp__.2008 crime',\n",
       "  '__nlp__.2008 living wayoflife',\n",
       "  '__nlp__.2008 message',\n",
       "  '__nlp__.2008 message date',\n",
       "  '__nlp__.2008 message id',\n",
       "  '__nlp__.2008 showbiz',\n",
       "  '__nlp__.2008 showbiz movies',\n",
       "  '__nlp__.2008 tech',\n",
       "  '__nlp__.2008 us',\n",
       "  '__nlp__.2008 us 08',\n",
       "  '__nlp__.2008 world',\n",
       "  '__nlp__.2079970',\n",
       "  '__nlp__.21',\n",
       "  '__nlp__.21 2008',\n",
       "  '__nlp__.211',\n",
       "  '__nlp__.22',\n",
       "  '__nlp__.23',\n",
       "  '__nlp__.235',\n",
       "  '__nlp__.24',\n",
       "  '__nlp__.24 10',\n",
       "  '__nlp__.24 10 27',\n",
       "  '__nlp__.24 10 30',\n",
       "  '__nlp__.24 10 31',\n",
       "  '__nlp__.24 2008',\n",
       "  '__nlp__.24 hours',\n",
       "  '__nlp__.24 hours to',\n",
       "  '__nlp__.25',\n",
       "  '__nlp__.25 2008',\n",
       "  '__nlp__.250',\n",
       "  '__nlp__.2520262',\n",
       "  '__nlp__.255',\n",
       "  '__nlp__.26',\n",
       "  '__nlp__.26 15',\n",
       "  '__nlp__.26 15 49',\n",
       "  '__nlp__.26 15 50',\n",
       "  '__nlp__.26 2008',\n",
       "  '__nlp__.269',\n",
       "  '__nlp__.27',\n",
       "  '__nlp__.27 2008',\n",
       "  '__nlp__.28',\n",
       "  '__nlp__.28 2008',\n",
       "  '__nlp__.29',\n",
       "  '__nlp__.29 2008',\n",
       "  '__nlp__.2gb',\n",
       "  '__nlp__.2nd',\n",
       "  '__nlp__.2to3',\n",
       "  '__nlp__.30',\n",
       "  '__nlp__.300',\n",
       "  '__nlp__.3000',\n",
       "  '__nlp__.3000 40tangomu',\n",
       "  '__nlp__.3000 mailing',\n",
       "  '__nlp__.3000 python',\n",
       "  '__nlp__.3000 python 3000',\n",
       "  '__nlp__.30th',\n",
       "  '__nlp__.30th of',\n",
       "  '__nlp__.30th of october',\n",
       "  '__nlp__.30th of september',\n",
       "  '__nlp__.31',\n",
       "  '__nlp__.32',\n",
       "  '__nlp__.32 bit',\n",
       "  '__nlp__.33',\n",
       "  '__nlp__.34',\n",
       "  '__nlp__.35',\n",
       "  '__nlp__.36',\n",
       "  '__nlp__.37',\n",
       "  '__nlp__.38',\n",
       "  '__nlp__.39',\n",
       "  '__nlp__.3d',\n",
       "  '__nlp__.3rd',\n",
       "  '__nlp__.40',\n",
       "  '__nlp__.40 of',\n",
       "  '__nlp__.4096',\n",
       "  '__nlp__.40cognosys',\n",
       "  '__nlp__.40tangomu',\n",
       "  '__nlp__.41',\n",
       "  '__nlp__.42',\n",
       "  '__nlp__.43',\n",
       "  '__nlp__.44',\n",
       "  '__nlp__.45',\n",
       "  '__nlp__.46',\n",
       "  '__nlp__.47',\n",
       "  '__nlp__.48',\n",
       "  '__nlp__.49',\n",
       "  '__nlp__.4th',\n",
       "  '__nlp__.50',\n",
       "  '__nlp__.500',\n",
       "  '__nlp__.51',\n",
       "  '__nlp__.52',\n",
       "  '__nlp__.53',\n",
       "  '__nlp__.54',\n",
       "  '__nlp__.55',\n",
       "  '__nlp__.550',\n",
       "  '__nlp__.56',\n",
       "  '__nlp__.57',\n",
       "  '__nlp__.58',\n",
       "  '__nlp__.58 pm',\n",
       "  '__nlp__.59',\n",
       "  '__nlp__.60',\n",
       "  '__nlp__.61',\n",
       "  '__nlp__.63',\n",
       "  '__nlp__.64',\n",
       "  '__nlp__.64 bit',\n",
       "  '__nlp__.64bit',\n",
       "  '__nlp__.65',\n",
       "  '__nlp__.66',\n",
       "  '__nlp__.67',\n",
       "  '__nlp__.69',\n",
       "  '__nlp__.70',\n",
       "  '__nlp__.72',\n",
       "  '__nlp__.75',\n",
       "  '__nlp__.78',\n",
       "  '__nlp__.79',\n",
       "  '__nlp__.7bit',\n",
       "  '__nlp__.80',\n",
       "  '__nlp__.800',\n",
       "  '__nlp__.802',\n",
       "  '__nlp__.83',\n",
       "  '__nlp__.85',\n",
       "  '__nlp__.8859',\n",
       "  '__nlp__.8859 format',\n",
       "  '__nlp__.8859 format flowed',\n",
       "  '__nlp__.89',\n",
       "  '__nlp__.90',\n",
       "  '__nlp__.911',\n",
       "  '__nlp__.911 call',\n",
       "  '__nlp__.93',\n",
       "  '__nlp__.94',\n",
       "  '__nlp__.95',\n",
       "  '__nlp__.9552',\n",
       "  '__nlp__.97',\n",
       "  '__nlp__.98',\n",
       "  '__nlp__.99',\n",
       "  '__nlp__.99 http',\n",
       "  '__nlp__.9a',\n",
       "  '__nlp__.__',\n",
       "  '__nlp__.______________________________________________',\n",
       "  '__nlp__._______________________________________________',\n",
       "  '__nlp__._______________________________________________ python',\n",
       "  '__nlp__._______________________________________________ python dev',\n",
       "  '__nlp__._______________________________________________ python win32',\n",
       "  '__nlp__._______________________________________________ wekalist',\n",
       "  '__nlp__.__future__',\n",
       "  '__nlp__.__init__',\n",
       "  '__nlp__.a1',\n",
       "  '__nlp__.aa',\n",
       "  '__nlp__.aaai',\n",
       "  '__nlp__.aaas',\n",
       "  '__nlp__.aaron',\n",
       "  '__nlp__.aaron kulkis',\n",
       "  '__nlp__.ab',\n",
       "  '__nlp__.abc',\n",
       "  '__nlp__.ability',\n",
       "  '__nlp__.ability to',\n",
       "  '__nlp__.able',\n",
       "  '__nlp__.able to',\n",
       "  '__nlp__.about',\n",
       "  '__nlp__.about it',\n",
       "  '__nlp__.about the',\n",
       "  '__nlp__.about this',\n",
       "  '__nlp__.about your',\n",
       "  '__nlp__.about your health',\n",
       "  '__nlp__.above',\n",
       "  '__nlp__.absolutely',\n",
       "  '__nlp__.absolutely guaranteed',\n",
       "  '__nlp__.abstract',\n",
       "  '__nlp__.abstracts',\n",
       "  '__nlp__.absurd',\n",
       "  '__nlp__.abuse',\n",
       "  '__nlp__.ac',\n",
       "  '__nlp__.ac nz',\n",
       "  '__nlp__.ac nz mailman',\n",
       "  '__nlp__.ac uk',\n",
       "  '__nlp__.academic',\n",
       "  '__nlp__.academy',\n",
       "  '__nlp__.accept',\n",
       "  '__nlp__.acceptance',\n",
       "  '__nlp__.accepted',\n",
       "  '__nlp__.accepted papers',\n",
       "  '__nlp__.accepted papers will',\n",
       "  '__nlp__.access',\n",
       "  '__nlp__.access to',\n",
       "  '__nlp__.accessible',\n",
       "  '__nlp__.according',\n",
       "  '__nlp__.according to the',\n",
       "  '__nlp__.account',\n",
       "  '__nlp__.account and',\n",
       "  '__nlp__.account and update',\n",
       "  '__nlp__.accounts',\n",
       "  '__nlp__.achieve',\n",
       "  '__nlp__.achieved',\n",
       "  '__nlp__.achieved the',\n",
       "  '__nlp__.acm',\n",
       "  '__nlp__.acm org',\n",
       "  '__nlp__.acpi',\n",
       "  '__nlp__.across',\n",
       "  '__nlp__.across the',\n",
       "  '__nlp__.act',\n",
       "  '__nlp__.action',\n",
       "  '__nlp__.actions',\n",
       "  '__nlp__.active',\n",
       "  '__nlp__.activities',\n",
       "  '__nlp__.activity',\n",
       "  '__nlp__.actual',\n",
       "  '__nlp__.actually',\n",
       "  '__nlp__.acy',\n",
       "  '__nlp__.ad',\n",
       "  '__nlp__.adam',\n",
       "  '__nlp__.adaptive',\n",
       "  '__nlp__.add',\n",
       "  '__nlp__.add inches',\n",
       "  '__nlp__.add inches to',\n",
       "  '__nlp__.added',\n",
       "  '__nlp__.added no',\n",
       "  '__nlp__.added no submission',\n",
       "  '__nlp__.added no virus',\n",
       "  '__nlp__.added to',\n",
       "  '__nlp__.added trojan',\n",
       "  '__nlp__.adding',\n",
       "  '__nlp__.addition',\n",
       "  '__nlp__.addition to',\n",
       "  '__nlp__.additional',\n",
       "  '__nlp__.additional commands',\n",
       "  '__nlp__.additional comments',\n",
       "  '__nlp__.address',\n",
       "  '__nlp__.address book',\n",
       "  '__nlp__.addressed',\n",
       "  '__nlp__.addresses',\n",
       "  '__nlp__.adds',\n",
       "  '__nlp__.admin',\n",
       "  '__nlp__.administration',\n",
       "  '__nlp__.administrator',\n",
       "  '__nlp__.adobe',\n",
       "  '__nlp__.ads',\n",
       "  '__nlp__.ads from',\n",
       "  '__nlp__.advance',\n",
       "  '__nlp__.advanced',\n",
       "  '__nlp__.advances',\n",
       "  '__nlp__.advantage',\n",
       "  '__nlp__.advantage of',\n",
       "  '__nlp__.advertise',\n",
       "  '__nlp__.advertisement',\n",
       "  '__nlp__.advertising',\n",
       "  '__nlp__.advice',\n",
       "  '__nlp__.adwords',\n",
       "  '__nlp__.aew45',\n",
       "  '__nlp__.affect',\n",
       "  '__nlp__.afraid',\n",
       "  '__nlp__.africa',\n",
       "  '__nlp__.after',\n",
       "  '__nlp__.after an',\n",
       "  '__nlp__.after the',\n",
       "  '__nlp__.after years',\n",
       "  '__nlp__.afternoon',\n",
       "  '__nlp__.again',\n",
       "  '__nlp__.against',\n",
       "  '__nlp__.against the',\n",
       "  '__nlp__.age',\n",
       "  '__nlp__.agent',\n",
       "  '__nlp__.agent systems',\n",
       "  '__nlp__.agents',\n",
       "  '__nlp__.ago',\n",
       "  '__nlp__.agree',\n",
       "  '__nlp__.agree with',\n",
       "  '__nlp__.agreed',\n",
       "  '__nlp__.agreed to',\n",
       "  '__nlp__.agreement',\n",
       "  '__nlp__.ahead',\n",
       "  '__nlp__.ai',\n",
       "  '__nlp__.aid',\n",
       "  '__nlp__.aid you',\n",
       "  '__nlp__.aim',\n",
       "  '__nlp__.aims',\n",
       "  '__nlp__.air',\n",
       "  '__nlp__.airlines',\n",
       "  '__nlp__.airport',\n",
       "  '__nlp__.ajax',\n",
       "  '__nlp__.al',\n",
       "  '__nlp__.albany',\n",
       "  '__nlp__.alert',\n",
       "  '__nlp__.alert alert',\n",
       "  '__nlp__.alert criteria',\n",
       "  '__nlp__.alerts',\n",
       "  '__nlp__.alerts my',\n",
       "  '__nlp__.alex',\n",
       "  '__nlp__.algorithm',\n",
       "  '__nlp__.algorithms',\n",
       "  '__nlp__.alias',\n",
       "  '__nlp__.alias trojan',\n",
       "  '__nlp__.aliases',\n",
       "  '__nlp__.aliasing',\n",
       "  '__nlp__.aliasing pipe',\n",
       "  '__nlp__.all',\n",
       "  '__nlp__.all gains',\n",
       "  '__nlp__.all in',\n",
       "  '__nlp__.all is',\n",
       "  '__nlp__.all of',\n",
       "  '__nlp__.all of the',\n",
       "  '__nlp__.all over',\n",
       "  '__nlp__.all over the',\n",
       "  '__nlp__.all results',\n",
       "  '__nlp__.all rights',\n",
       "  '__nlp__.all that',\n",
       "  '__nlp__.all the',\n",
       "  '__nlp__.all you',\n",
       "  '__nlp__.all your',\n",
       "  '__nlp__.allow',\n",
       "  '__nlp__.allow you',\n",
       "  '__nlp__.allowed',\n",
       "  '__nlp__.allowing',\n",
       "  '__nlp__.allows',\n",
       "  '__nlp__.almost',\n",
       "  '__nlp__.alone',\n",
       "  '__nlp__.along',\n",
       "  '__nlp__.along with',\n",
       "  '__nlp__.alpha',\n",
       "  '__nlp__.alphas',\n",
       "  '__nlp__.already',\n",
       "  '__nlp__.already detected',\n",
       "  '__nlp__.also',\n",
       "  '__nlp__.also be',\n",
       "  '__nlp__.also the',\n",
       "  '__nlp__.also thicker',\n",
       "  '__nlp__.alt',\n",
       "  '__nlp__.alter',\n",
       "  '__nlp__.alternative',\n",
       "  '__nlp__.alternatives',\n",
       "  '__nlp__.although',\n",
       "  '__nlp__.always',\n",
       "  '__nlp__.am',\n",
       "  '__nlp__.am not',\n",
       "  '__nlp__.am trying',\n",
       "  '__nlp__.am using',\n",
       "  '__nlp__.amavis',\n",
       "  '__nlp__.amazing',\n",
       "  '__nlp__.amazing growth',\n",
       "  '__nlp__.amazon',\n",
       "  '__nlp__.amazon com',\n",
       "  '__nlp__.amd',\n",
       "  '__nlp__.amd64',\n",
       "  '__nlp__.america',\n",
       "  '__nlp__.american',\n",
       "  '__nlp__.americas',\n",
       "  '__nlp__.among',\n",
       "  '__nlp__.among the',\n",
       "  '__nlp__.amount',\n",
       "  '__nlp__.amount of',\n",
       "  '__nlp__.amsterdam',\n",
       "  '__nlp__.amy',\n",
       "  '__nlp__.an',\n",
       "  '__nlp__.an american',\n",
       "  '__nlp__.an average',\n",
       "  '__nlp__.an economic',\n",
       "  '__nlp__.an email',\n",
       "  '__nlp__.an error',\n",
       "  '__nlp__.an example',\n",
       "  '__nlp__.an html',\n",
       "  '__nlp__.an increase',\n",
       "  '__nlp__.an increase in',\n",
       "  '__nlp__.an incredible',\n",
       "  '__nlp__.an issue',\n",
       "  '__nlp__.an official',\n",
       "  '__nlp__.an open',\n",
       "  '__nlp__.analysis',\n",
       "  '__nlp__.analysis of',\n",
       "  '__nlp__.analyst',\n",
       "  '__nlp__.and',\n",
       "  '__nlp__.and achieve',\n",
       "  '__nlp__.and all',\n",
       "  '__nlp__.and allow',\n",
       "  '__nlp__.and also',\n",
       "  '__nlp__.and am',\n",
       "  '__nlp__.and an',\n",
       "  '__nlp__.and any',\n",
       "  '__nlp__.and applications',\n",
       "  '__nlp__.and are',\n",
       "  '__nlp__.and are very',\n",
       "  '__nlp__.and as',\n",
       "  '__nlp__.and be',\n",
       "  '__nlp__.and can',\n",
       "  '__nlp__.and change',\n",
       "  '__nlp__.and change your',\n",
       "  '__nlp__.and come',\n",
       "  '__nlp__.and data',\n",
       "  '__nlp__.and do',\n",
       "  '__nlp__.and don',\n",
       "  '__nlp__.and even',\n",
       "  '__nlp__.and events',\n",
       "  '__nlp__.and for',\n",
       "  '__nlp__.and get',\n",
       "  '__nlp__.and girth',\n",
       "  '__nlp__.and harmony',\n",
       "  '__nlp__.and has',\n",
       "  '__nlp__.and have',\n",
       "  '__nlp__.and he',\n",
       "  '__nlp__.and how',\n",
       "  '__nlp__.and if',\n",
       "  '__nlp__.and in',\n",
       "  '__nlp__.and information',\n",
       "  '__nlp__.and is',\n",
       "  '__nlp__.and it',\n",
       "  '__nlp__.and its',\n",
       "  '__nlp__.and knowledge',\n",
       "  '__nlp__.and learning',\n",
       "  '__nlp__.and length',\n",
       "  '__nlp__.and make',\n",
       "  '__nlp__.and may',\n",
       "  '__nlp__.and more',\n",
       "  '__nlp__.and my',\n",
       "  '__nlp__.and not',\n",
       "  '__nlp__.and now',\n",
       "  '__nlp__.and on',\n",
       "  '__nlp__.and one',\n",
       "  '__nlp__.and only',\n",
       "  '__nlp__.and or',\n",
       "  '__nlp__.and other',\n",
       "  '__nlp__.and provide',\n",
       "  '__nlp__.and save',\n",
       "  '__nlp__.and see',\n",
       "  '__nlp__.and she',\n",
       "  '__nlp__.and she comes',\n",
       "  '__nlp__.and should',\n",
       "  '__nlp__.and so',\n",
       "  '__nlp__.and some',\n",
       "  '__nlp__.and technology',\n",
       "  '__nlp__.and that',\n",
       "  '__nlp__.and the',\n",
       "  '__nlp__.and their',\n",
       "  '__nlp__.and then',\n",
       "  '__nlp__.and there',\n",
       "  '__nlp__.and they',\n",
       "  '__nlp__.and think',\n",
       "  '__nlp__.and think about',\n",
       "  '__nlp__.and this',\n",
       "  '__nlp__.and to',\n",
       "  '__nlp__.and unfortunately',\n",
       "  '__nlp__.and update',\n",
       "  '__nlp__.and update your',\n",
       "  '__nlp__.and use',\n",
       "  '__nlp__.and was',\n",
       "  '__nlp__.and watch',\n",
       "  '__nlp__.and we',\n",
       "  '__nlp__.and we are',\n",
       "  '__nlp__.and what',\n",
       "  '__nlp__.and when',\n",
       "  '__nlp__.and will',\n",
       "  '__nlp__.and with',\n",
       "  '__nlp__.and would',\n",
       "  '__nlp__.and you',\n",
       "  '__nlp__.and you can',\n",
       "  '__nlp__.and your',\n",
       "  '__nlp__.anders',\n",
       "  '__nlp__.andreas',\n",
       "  '__nlp__.andrew',\n",
       "  '__nlp__.andy',\n",
       "  '__nlp__.angeles',\n",
       "  '__nlp__.ann',\n",
       "  '__nlp__.announce',\n",
       "  '__nlp__.announced',\n",
       "  '__nlp__.announcement',\n",
       "  '__nlp__.announcements',\n",
       "  '__nlp__.annual',\n",
       "  '__nlp__.anonymous',\n",
       "  '__nlp__.another',\n",
       "  '__nlp__.answer',\n",
       "  '__nlp__.answerdry',\n",
       "  '__nlp__.answers',\n",
       "  '__nlp__.anthony',\n",
       "  '__nlp__.anthrax',\n",
       "  '__nlp__.anthrax suspect',\n",
       "  '__nlp__.anti',\n",
       "  '__nlp__.any',\n",
       "  '__nlp__.any of',\n",
       "  '__nlp__.any of the',\n",
       "  '__nlp__.any other',\n",
       "  '__nlp__.any way',\n",
       "  '__nlp__.anybody',\n",
       "  '__nlp__.anymore',\n",
       "  '__nlp__.anyone',\n",
       "  '__nlp__.anyone else',\n",
       "  '__nlp__.anyone know',\n",
       "  '__nlp__.anything',\n",
       "  '__nlp__.anyway',\n",
       "  '__nlp__.anywhere',\n",
       "  '__nlp__.aol',\n",
       "  '__nlp__.aol com',\n",
       "  '__nlp__.ap',\n",
       "  '__nlp__.ap index',\n",
       "  '__nlp__.apache',\n",
       "  '__nlp__.apache org',\n",
       "  '__nlp__.apache org bug',\n",
       "  '__nlp__.apache org by',\n",
       "  '__nlp__.apache org helo',\n",
       "  '__nlp__.apache org spamassassin',\n",
       "  '__nlp__.api',\n",
       "  '__nlp__.apologies',\n",
       "  '__nlp__.apologies for',\n",
       "  '__nlp__.app',\n",
       "  '__nlp__.apparent',\n",
       "  '__nlp__.apparently',\n",
       "  '__nlp__.appeal',\n",
       "  '__nlp__.appear',\n",
       "  '__nlp__.appear in',\n",
       "  '__nlp__.appears',\n",
       "  '__nlp__.appears to',\n",
       "  '__nlp__.apple',\n",
       "  '__nlp__.apple com',\n",
       "  '__nlp__.applicable',\n",
       "  '__nlp__.application',\n",
       "  '__nlp__.applications',\n",
       "  '__nlp__.applications and',\n",
       "  '__nlp__.applications of',\n",
       "  '__nlp__.applied',\n",
       "  '__nlp__.applied to',\n",
       "  '__nlp__.applies',\n",
       "  '__nlp__.apply',\n",
       "  '__nlp__.applying',\n",
       "  '__nlp__.appreciate',\n",
       "  '__nlp__.appreciated',\n",
       "  '__nlp__.approach',\n",
       "  '__nlp__.approach to',\n",
       "  '__nlp__.approaches',\n",
       "  '__nlp__.appropriate',\n",
       "  '__nlp__.approved',\n",
       "  '__nlp__.apps',\n",
       "  '__nlp__.apr',\n",
       "  '__nlp__.apr 2008',\n",
       "  '__nlp__.apr 2008 at',\n",
       "  '__nlp__.apr 26',\n",
       "  '__nlp__.april',\n",
       "  '__nlp__.april 2008',\n",
       "  '__nlp__.ar',\n",
       "  '__nlp__.aran',\n",
       "  '__nlp__.architect',\n",
       "  '__nlp__.architecture',\n",
       "  '__nlp__.architectures',\n",
       "  '__nlp__.archive',\n",
       "  '__nlp__.archives',\n",
       "  '__nlp__.are',\n",
       "  '__nlp__.are 100',\n",
       "  '__nlp__.are all',\n",
       "  '__nlp__.are also',\n",
       "  '__nlp__.are available',\n",
       "  '__nlp__.are being',\n",
       "  '__nlp__.are currently',\n",
       "  '__nlp__.are glad',\n",
       "  '__nlp__.are guaranteed',\n",
       "  '__nlp__.are in',\n",
       "  '__nlp__.are invited',\n",
       "  '__nlp__.are invited to',\n",
       "  '__nlp__.are just',\n",
       "  '__nlp__.are looking',\n",
       "  '__nlp__.are looking for',\n",
       "  '__nlp__.are no',\n",
       "  '__nlp__.are not',\n",
       "  '__nlp__.are not limited',\n",
       "  '__nlp__.are now',\n",
       "  '__nlp__.are receiving',\n",
       "  '__nlp__.are receiving this',\n",
       "  '__nlp__.are still',\n",
       "  '__nlp__.are subscribed',\n",
       "  '__nlp__.are the',\n",
       "  '__nlp__.are the assignee',\n",
       "  '__nlp__.are to',\n",
       "  '__nlp__.are trying',\n",
       "  '__nlp__.are using',\n",
       "  '__nlp__.are very',\n",
       "  '__nlp__.are watching',\n",
       "  '__nlp__.are you',\n",
       "  '__nlp__.area',\n",
       "  '__nlp__.areas',\n",
       "  '__nlp__.areas of',\n",
       "  '__nlp__.aren',\n",
       "  '__nlp__.args',\n",
       "  '__nlp__.argument',\n",
       "  '__nlp__.argumentation',\n",
       "  '__nlp__.arguments',\n",
       "  '__nlp__.arm',\n",
       "  '__nlp__.around',\n",
       "  '__nlp__.around the',\n",
       "  '__nlp__.around the world',\n",
       "  '__nlp__.array',\n",
       "  '__nlp__.arrest',\n",
       "  '__nlp__.arrested',\n",
       "  '__nlp__.art',\n",
       "  '__nlp__.article',\n",
       "  '__nlp__.article pl',\n",
       "  '__nlp__.articles',\n",
       "  '__nlp__.artificial',\n",
       "  '__nlp__.artificial intelligence',\n",
       "  '__nlp__.as',\n",
       "  '__nlp__.as an',\n",
       "  '__nlp__.as by',\n",
       "  '__nlp__.as far',\n",
       "  '__nlp__.as far as',\n",
       "  '__nlp__.as fast',\n",
       "  '__nlp__.as follows',\n",
       "  '__nlp__.as in',\n",
       "  '__nlp__.as in submission',\n",
       "  '__nlp__.as it',\n",
       "  '__nlp__.as long',\n",
       "  '__nlp__.as long as',\n",
       "  '__nlp__.as mouse',\n",
       "  '__nlp__.as much',\n",
       "  '__nlp__.as of',\n",
       "  '__nlp__.as part',\n",
       "  '__nlp__.as possible',\n",
       "  '__nlp__.as result',\n",
       "  '__nlp__.as result of',\n",
       "  '__nlp__.as soon',\n",
       "  '__nlp__.as the',\n",
       "  '__nlp__.as they',\n",
       "  '__nlp__.as to',\n",
       "  '__nlp__.as trojan',\n",
       "  '__nlp__.as trojan agent',\n",
       "  '__nlp__.as trojan downloader',\n",
       "  '__nlp__.as trojan spy',\n",
       "  '__nlp__.as we',\n",
       "  '__nlp__.as well',\n",
       "  '__nlp__.as well as',\n",
       "  '__nlp__.as you',\n",
       "  '__nlp__.ascii',\n",
       "  '__nlp__.ask',\n",
       "  '__nlp__.asked',\n",
       "  '__nlp__.asked to',\n",
       "  '__nlp__.asking',\n",
       "  '__nlp__.asks',\n",
       "  '__nlp__.asp',\n",
       "  '__nlp__.aspects',\n",
       "  '__nlp__.aspects of',\n",
       "  '__nlp__.aspx',\n",
       "  '__nlp__.assertion',\n",
       "  '__nlp__.assigned',\n",
       "  '__nlp__.assignee',\n",
       "  '__nlp__.assignee for',\n",
       "  '__nlp__.assist',\n",
       "  '__nlp__.assist you',\n",
       "  '__nlp__.assistant',\n",
       "  '__nlp__.associate',\n",
       "  '__nlp__.associated',\n",
       "  '__nlp__.associated with',\n",
       "  '__nlp__.association',\n",
       "  '__nlp__.assume',\n",
       "  '__nlp__.assuming',\n",
       "  '__nlp__.astrology',\n",
       "  '__nlp__.astrology com',\n",
       "  '__nlp__.at',\n",
       "  '__nlp__.at 10',\n",
       "  '__nlp__.at 11',\n",
       "  '__nlp__.at 12',\n",
       "  '__nlp__.at all',\n",
       "  '__nlp__.at home',\n",
       "  '__nlp__.at http',\n",
       "  '__nlp__.at http groups',\n",
       "  '__nlp__.at http twitter',\n",
       "  '__nlp__.at http www',\n",
       "  '__nlp__.at http yro',\n",
       "  '__nlp__.at least',\n",
       "  '__nlp__.at least one',\n",
       "  '__nlp__.at nabble',\n",
       "  '__nlp__.at replica',\n",
       "  '__nlp__.at replica classics',\n",
       "  '__nlp__.at some',\n",
       "  '__nlp__.at that',\n",
       "  '__nlp__.at the',\n",
       "  '__nlp__.at the end',\n",
       "  '__nlp__.at the moment',\n",
       "  '__nlp__.at the same',\n",
       "  '__nlp__.at this',\n",
       "  '__nlp__.at work',\n",
       "  '__nlp__.at work and',\n",
       "  '__nlp__.at your',\n",
       "  '__nlp__.atches',\n",
       "  '__nlp__.atlanta',\n",
       "  '__nlp__.atlanta georgia',\n",
       "  '__nlp__.attached',\n",
       "  '__nlp__.attachment',\n",
       "  '__nlp__.attachment was',\n",
       "  '__nlp__.attachments',\n",
       "  '__nlp__.attack',\n",
       "  '__nlp__.attacks',\n",
       "  '__nlp__.attempt',\n",
       "  '__nlp__.attempt to',\n",
       "  '__nlp__.attend',\n",
       "  '__nlp__.attention',\n",
       "  '__nlp__.attribute',\n",
       "  '__nlp__.attributes',\n",
       "  '__nlp__.au',\n",
       "  '__nlp__.auckland',\n",
       "  '__nlp__.audience',\n",
       "  '__nlp__.audio',\n",
       "  '__nlp__.aug',\n",
       "  '__nlp__.aug 2008',\n",
       "  '__nlp__.aug 2008 05',\n",
       "  '__nlp__.aug 2008 11',\n",
       "  '__nlp__.august',\n",
       "  '__nlp__.australia',\n",
       "  '__nlp__.austria',\n",
       "  '__nlp__.auth',\n",
       "  '__nlp__.auth msa',\n",
       "  '__nlp__.auth msa ip',\n",
       "  '__nlp__.authentication',\n",
       "  '__nlp__.author',\n",
       "  '__nlp__.authors',\n",
       "  '__nlp__.authors are',\n",
       "  '__nlp__.auto',\n",
       "  '__nlp__.autolearn',\n",
       "  '__nlp__.automated',\n",
       "  '__nlp__.automatic',\n",
       "  '__nlp__.automatically',\n",
       "  '__nlp__.availability',\n",
       "  '__nlp__.available',\n",
       "  '__nlp__.available at',\n",
       "  '__nlp__.available at http',\n",
       "  '__nlp__.available for',\n",
       "  '__nlp__.available in',\n",
       "  '__nlp__.available on',\n",
       "  '__nlp__.available on the',\n",
       "  '__nlp__.available to',\n",
       "  '__nlp__.avenue',\n",
       "  '__nlp__.average',\n",
       "  '__nlp__.avg',\n",
       "  '__nlp__.avg free',\n",
       "  '__nlp__.avoid',\n",
       "  '__nlp__.award',\n",
       "  '__nlp__.awards',\n",
       "  '__nlp__.aware',\n",
       "  '__nlp__.aware of',\n",
       "  '__nlp__.away',\n",
       "  '__nlp__.awl',\n",
       "  '__nlp__.back',\n",
       "  '__nlp__.back at',\n",
       "  '__nlp__.back to',\n",
       "  '__nlp__.backdoor',\n",
       "  '__nlp__.backend',\n",
       "  '__nlp__.background',\n",
       "  '__nlp__.backup',\n",
       "  '__nlp__.bad',\n",
       "  '__nlp__.bag',\n",
       "  '__nlp__.bank',\n",
       "  '__nlp__.bar',\n",
       "  '__nlp__.barry',\n",
       "  '__nlp__.barry warsaw',\n",
       "  '__nlp__.base',\n",
       "  '__nlp__.based',\n",
       "  '__nlp__.based on',\n",
       "  '__nlp__.based on the',\n",
       "  '__nlp__.bash',\n",
       "  '__nlp__.basic',\n",
       "  '__nlp__.basically',\n",
       "  '__nlp__.basis',\n",
       "  '__nlp__.bat',\n",
       "  '__nlp__.battle',\n",
       "  ...],\n",
       " 'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x18549ac9c10>,\n",
       " 'memory_size': 30393236,\n",
       " 'compile_time': None,\n",
       " 'is_initialized': True,\n",
       " 'is_fit': True,\n",
       " 'is_valid': True,\n",
       " 'can_infer': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info['model_info']['RandomForestGini']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
