{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a198467-b7c1-4f6b-aa90-649d13b94a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime, time\n",
    "from botocore.client import ClientError\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82af36-177e-4ccb-a97c-62e6e65c6472",
   "metadata": {},
   "source": [
    "### Set up Region, Session, Bucket, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674ac040-fec4-4074-92e7-5cb0b398a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-us-east-1-019877554860\n"
     ]
    }
   ],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3 = boto3.Session().client(service_name=\"s3\", region_name=region)\n",
    "role = get_execution_role()\n",
    "prefix = \"spam-detection\"\n",
    "s3_key = f\"s3://{bucket}/{prefix}\"\n",
    "script_path = sagemaker.Session().upload_data(\n",
    "    path=\"spam_detection.py\", \n",
    "    bucket=bucket, \n",
    "    key_prefix='scripts'\n",
    ")\n",
    "\n",
    "print(\"Default bucket: {}\".format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed253762-c3ba-49be-af70-a34d3477df81",
   "metadata": {},
   "source": [
    "### Confirm bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c7dc46-8647-4396-b25b-1c88e6629af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'XG4VSBG9V37DN90B', 'HostId': 'oDyZndjAprvNJYyG1CLya+omujYoHa5cpj/8PTRnCoWilo2C0FM9CDnFW7ucTF5FPhiEqb93bf4=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'oDyZndjAprvNJYyG1CLya+omujYoHa5cpj/8PTRnCoWilo2C0FM9CDnFW7ucTF5FPhiEqb93bf4=', 'x-amz-request-id': 'XG4VSBG9V37DN90B', 'date': 'Wed, 16 Oct 2024 05:52:25 GMT', 'x-amz-bucket-region': 'us-east-1', 'x-amz-access-point-alias': 'false', 'content-type': 'application/xml', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'BucketRegion': 'us-east-1', 'AccessPointAlias': False}\n"
     ]
    }
   ],
   "source": [
    "response = None\n",
    "try:\n",
    "    response = s3.head_bucket(Bucket=bucket)\n",
    "    print(response)\n",
    "    setup_s3_bucket_passed = True\n",
    "except ClientError as e:\n",
    "    print(\"[ERROR] Cannot find bucket {} in {} due to {}.\".format(bucket, response, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77493a5-74e4-4d6a-9b88-24e44cbfa1ee",
   "metadata": {},
   "source": [
    "### Load the dataset. Drop Receive, Date, URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc26098d-418e-4556-9b2f-962fed872d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'CEAS_08.csv'\n",
    "local_csv_path = 'dataset/CEAS_08.csv'\n",
    "\n",
    "df = pd.read_csv(local_csv_path)\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "df = df.drop([\"receiver\",\"date\",\"urls\"], axis=1)\n",
    "df[\"body\"] = df[\"body\"].str.replace(r'\\n', ' ')\n",
    "\n",
    "folder_in_s3 = 'Dataset/'\n",
    "s3_destination_dir = f's3://{bucket}/{folder_in_s3}'\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83b5c0a-c444-4620-8488-81d8535d49dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 'body' column: 0\n",
      "Number of NaN values in 'body' column: 0\n",
      "Number of NaN values in 'body' column: 0\n"
     ]
    }
   ],
   "source": [
    "num_missing = df['body'].isnull().sum()\n",
    "print(f\"Number of missing values in 'body' column: {num_missing}\")\n",
    "num_missing = df['body'].isna().sum()\n",
    "print(f\"Number of NaN values in 'body' column: {num_missing}\")\n",
    "total_rows = len(df)\n",
    "non_null_count = df['body'].notnull().sum()\n",
    "num_missing = total_rows - non_null_count\n",
    "\n",
    "print(f\"Number of NaN values in 'body' column: {num_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f01339d-5695-49a6-b4a1-0d1c14e13cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 31323 rows\n",
      "Test data: 3915 rows\n",
      "Validation data: 3916 rows\n",
      "Production data: 7831 rows\n"
     ]
    }
   ],
   "source": [
    "# Splitting 80% for training, 10% for testing, 10% for validation/\n",
    "# First, allocate 80% to df_train and 20% to df_production.\n",
    "df_train, df_production = train_test_split(df, test_size=0.20, random_state=42)\n",
    "# Splitting df_production where 10% goes to df_test and 10% goes to df_validation.\n",
    "df_test, df_validation = train_test_split(df_production, test_size=0.50, random_state=42)\n",
    "\n",
    "# Print the sizes of each split to confirm\n",
    "print(f\"Training data: {len(df_train)} rows\")\n",
    "print(f\"Test data: {len(df_test)} rows\")\n",
    "print(f\"Validation data: {len(df_validation)} rows\")\n",
    "print(f\"Production data: {len(df_production)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cca5a05-d2d4-4264-9225-4c5205ba98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to: s3://sagemaker-us-east-1-019877554860/spam-detection/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the training data to CSV\n",
    "df_train[[\"sender\", \"subject\", \"body\", \"label\"]].to_csv('train.csv', index=False)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'spam-detection'\n",
    "\n",
    "train_input_path = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=f'{prefix}/train')\n",
    "\n",
    "print(f'Training data uploaded to: {train_input_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a809fad-cbb6-4074-a4cd-af44214a21dc",
   "metadata": {},
   "source": [
    "### Spam_detection.py trainiing script - Not to be run within Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d49368f-01f2-4f29-adac-dc7bbce77690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport argparse\\nimport os\\nimport pandas as pd\\nimport joblib\\n\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\ndef model_fn(model_dir):\\n    # Load the model for inference\\n    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\\n    vectorizer = joblib.load(os.path.join(model_dir, \"vectorizer.joblib\"))\\n    return model, vectorizer\\n\\ndef predict_fn(input_data, model_and_vectorizer):\\n    # Vectorize string input and make predictions\\n    model, vectorizer = model_and_vectorizer\\n    \\n    # Check if the input data is a string (email body)\\n    # Transform the input string to TF-IDF features\\n    input_tfidf = vectorizer.transform([str(input_data)])\\n    input_dense = input_tfidf.toarray()  # Convert to dense format for GaussianNB\\n\\n    # Make predictions using the trained model\\n    prediction = model.predict(input_dense)\\n    \\n    return prediction\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n    \\n    # SageMaker-specific arguments\\n    parser.add_argument(\\'--model-dir\\', type=str, default=os.environ.get(\\'SM_MODEL_DIR\\'))\\n    parser.add_argument(\\'--train\\', type=str, default=os.environ.get(\\'SM_CHANNEL_TRAIN\\'))\\n    \\n    args = parser.parse_args()\\n    \\n    # Read the training data\\n    train_data = pd.read_csv(os.path.join(args.train, \\'train.csv\\'))\\n    \\n    # Separate features and target\\n    X_train = train_data.drop(\"label\", axis=1).astype(str)\\n    y_train = train_data[\\'label\\']\\n    \\n    # Create the TF-IDF vectorizer\\n    vectorizer = TfidfVectorizer(max_features=10000)    \\n    X_train_tfidf = vectorizer.fit_transform(X_train)\\n   \\n    # Convert sparse matrix to dense for GaussianNB\\n    X_train_dense = X_train_tfidf.toarray()\\n    \\n    # Train the model\\n    # Build a Naive Bayes Classifier\\n    model = GaussianNB()\\n    \\n    model.fit(X_train_dense, y_train)\\n    \\n    # Save the model\\n    joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\\n    joblib.dump(vectorizer, os.path.join(args.model_dir, \"vectorizer.joblib\"))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spam_detection.py - Not to be run within the Jupyter Notebook.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # Load the model for inference\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    vectorizer = joblib.load(os.path.join(model_dir, \"vectorizer.joblib\"))\n",
    "    return model, vectorizer\n",
    "\n",
    "def predict_fn(input_data, model_and_vectorizer):\n",
    "    # Vectorize string input and make predictions\n",
    "    model, vectorizer = model_and_vectorizer\n",
    "    \n",
    "    # Check if the input data is a string (email body)\n",
    "    # Transform the input string to TF-IDF features\n",
    "    input_tfidf = vectorizer.transform([str(input_data)])\n",
    "    input_dense = input_tfidf.toarray()  # Convert to dense format for GaussianNB\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    prediction = model.predict(input_dense)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # SageMaker-specific arguments\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Read the training data\n",
    "    train_data = pd.read_csv(os.path.join(args.train, 'train.csv'))\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_data.drop(\"label\", axis=1).astype(str)\n",
    "    y_train = train_data['label']\n",
    "    \n",
    "    # Create the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=10000)    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "   \n",
    "    # Convert sparse matrix to dense for GaussianNB\n",
    "    X_train_dense = X_train_tfidf.toarray()\n",
    "    \n",
    "    # Train the model\n",
    "    # Build a Naive Bayes Classifier\n",
    "    model = GaussianNB()\n",
    "    \n",
    "    model.fit(X_train_dense, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    joblib.dump(vectorizer, os.path.join(args.model_dir, \"vectorizer.joblib\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dfcac-1a5d-4b24-98ce-6755980973f1",
   "metadata": {},
   "source": [
    "### Import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db49209-1bb6-462b-9a3f-6943fa6acf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearn\n",
    "import tempfile\n",
    "\n",
    "# Upload the script to Amazon S3\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "script_path = sagemaker.Session().upload_data(\n",
    "    path='spam_detection.py', \n",
    "    bucket=bucket, \n",
    "    key_prefix='scripts'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227b869-67f8-45fe-99fb-9459fec9b7d3",
   "metadata": {},
   "source": [
    "### Upload training data to the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d178a2-5c1d-45c8-94b4-16b5a9390289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local path to your training data directory\n",
    "train_local_path = 'train'  # Ensure 'train/train.csv' exists\n",
    "\n",
    "# Upload training data to S3\n",
    "train_data_path = sagemaker_session.upload_data(\n",
    "    path=train_local_path,\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}/train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26baf1a-ace0-47ff-87da-aae34fbeba23",
   "metadata": {},
   "source": [
    "### Create an estimator and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bf6d6-ce61-4f90-bb05-bafe5d88da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "estimator = SKLearn(\n",
    "    entry_point='spam_detection.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "    dependencies=['requirements.txt']\n",
    ")\n",
    "\n",
    "# Run the training job\n",
    "# estimator.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d2a3d-80bd-4a75-bfad-982351000446",
   "metadata": {},
   "source": [
    "### Create a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9215790c-0cae-41e2-a8a5-6ab9a406d0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2024-10-16-05-55-10-656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 05:55:12 Starting - Starting the training job...\n",
      "2024-10-16 05:55:26 Starting - Preparing the instances for training...\n",
      "2024-10-16 05:56:08 Downloading - Downloading the training image......\n",
      "2024-10-16 05:56:54 Training - Training image download completed. Training in progress.\u001b[34m2024-10-16 05:57:00,373 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:00,376 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:00,379 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:00,396 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:00,612 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /miniconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /miniconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.2.1)\u001b[0m\n",
      "\u001b[34mCollecting xgboost (from -r requirements.txt (line 3))\n",
      "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /miniconda3/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /miniconda3/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15.4 in /miniconda3/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (1.24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.3.2 in /miniconda3/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.5.0)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12 (from xgboost->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mDownloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153.9/153.9 MB 216.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\u001b[0m\n",
      "\u001b[34m   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.0/199.0 MB 143.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nvidia-nccl-cu12, xgboost\u001b[0m\n",
      "\u001b[34mSuccessfully installed nvidia-nccl-cu12-2.23.4 xgboost-2.1.1\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,693 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,696 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,714 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,716 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,733 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,736 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,751 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2024-10-16-05-55-10-656\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-019877554860/sagemaker-scikit-learn-2024-10-16-05-55-10-656/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"spam_detection\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"spam_detection.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=spam_detection.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=spam_detection\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-019877554860/sagemaker-scikit-learn-2024-10-16-05-55-10-656/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"sagemaker-scikit-learn-2024-10-16-05-55-10-656\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-019877554860/sagemaker-scikit-learn-2024-10-16-05-55-10-656/source/sourcedir.tar.gz\",\"module_name\":\"spam_detection\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"spam_detection.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python spam_detection.py\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,752 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-10-16 05:57:08,753 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\n",
      "2024-10-16 05:57:44 Uploading - Uploading generated training model\u001b[34m2024-10-16 05:57:39,189 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-10-16 05:57:52 Completed - Training job completed\n",
      "Training seconds: 125\n",
      "Billable seconds: 125\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# define endpoint name\n",
    "endpoint_name = f\"{prefix}-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "\n",
    "# Initialize the SKLearnModel with an explicit model name\n",
    "model = SKLearn(\n",
    "    model_data=f\"s3://{bucket}/{prefix}/output/sagemaker-scikit-learn-2024-10-16-03-58-23-641/output/model.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"spam_detection.py\",  # Your inference script\n",
    "    name=endpoint_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "    dependencies=['requirements.txt']\n",
    ")\n",
    "\n",
    "model.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a47ff80-70dc-4b4c-a341-90916321d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: spam-detection-2024-10-16-0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exists. Proceeding with Clarify job.\n"
     ]
    }
   ],
   "source": [
    "# Create a SageMaker Model entity\n",
    "model_name = 'spam-detection-2024-10-16-0558'  # Must match the model_name in ModelConfig\n",
    "\n",
    "sagemaker_model = model.create_model(\n",
    "    name=model_name,\n",
    "    entry_point=\"spam_detection.py\",\n",
    "    dependencies=['requirements.txt'],\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# **Add this line to create the model in SageMaker**\n",
    "sagemaker_model.create(\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")\n",
    "\n",
    "# Check to make sure the model was actually created.\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "# List existing models\n",
    "response = sagemaker_client.list_models(\n",
    "    NameContains='spam-detection-2024-10-16-0558'\n",
    ")\n",
    "\n",
    "if not response['Models']:\n",
    "    print(\"Model does not exist. Please create the model before proceeding.\")\n",
    "else:\n",
    "    print(\"Model exists. Ready for Clarify job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fa45f-1852-4c54-8431-15438ccaf3e1",
   "metadata": {},
   "source": [
    "### Create an endpoint and assign the model to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a9965a-700c-4b1f-83fe-d37f995e3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName = spam-detection-2024-10-16-0558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2024-10-16-05-58-28-114\n",
      "INFO:sagemaker:Creating endpoint-config with name spam-detection-2024-10-16-0558\n",
      "INFO:sagemaker:Creating endpoint with name spam-detection-2024-10-16-0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!Model successfully deployed to endpoint: spam-detection-2024-10-16-0558\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# define endpoint name\n",
    "endpoint_name = f\"{prefix}-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"EndpointName =\", endpoint_name)\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name   # Adding endpoint name for model monitoring purposes\n",
    ")\n",
    "print(f\"Model successfully deployed to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a902d35-75c6-4efc-8ee5-70d88bea58d1",
   "metadata": {},
   "source": [
    "### Start SHAP analysis - import the necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab7eebe-569c-4fdd-a339-f8d06ce67e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.clarify import (\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    SHAPConfig,\n",
    "    TextConfig,\n",
    "    SageMakerClarifyProcessor,\n",
    "    ModelPredictedLabelConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54332df-1c45-47d9-9648-85ada862b4cd",
   "metadata": {},
   "source": [
    "### Define feature columns. Set up the necessary configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7120ec6b-2eaf-4c15-af46-97398e5771c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.0.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\"sender\", \"subject\", \"body\"]\n",
    "features = \",\".join(feature_columns)\n",
    "\n",
    "# Set up the configurations for SageMaker Clarify\n",
    "data_config = DataConfig(\n",
    "    s3_data_input_path=train_data_path,\n",
    "    s3_output_path=f\"s3://{bucket}/{prefix}/clarify-explainability\",\n",
    "    label=\"label\",\n",
    "    headers=True,\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name=endpoint_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_path,\n",
    "    s3_output_path=f\"s3://{bucket}/{prefix}/clarify-explainability\",\n",
    "    label=\"label\",\n",
    "    headers=df_train.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "model_predicted_label_config = ModelPredictedLabelConfig(label=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce27a0-f386-43d7-8e2c-23803b3909eb",
   "metadata": {},
   "source": [
    "### Configure explainability DataConfig and SHAPConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a857816-2bb4-4d00-a916-6c1c52cb58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainability_output_path = f\"s3://{bucket}/{prefix}/clarify-explainability\"\n",
    "explainability_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_path,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    label=\"label\",\n",
    "    headers=df_train.columns.tolist(),\n",
    "    dataset_type=\"text/csv\",\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "# SHAP configuration\n",
    "shap_config = SHAPConfig(\n",
    "    baseline=[\n",
    "        [\"larry@google.com\", \"I have your data\", \"Turn on your cookies.\"],\n",
    "    ],\n",
    "    num_samples=15,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd54348-44a4-434e-a019-8493d59406e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model does not exist. Please create the model before proceeding.\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "# List existing models\n",
    "response = sagemaker_client.list_models(\n",
    "    NameContains='spam-detection-2024-10-16-0558'\n",
    ")\n",
    "\n",
    "if not response['Models']:\n",
    "    print(\"Model does not exist. Please create the model before proceeding.\")\n",
    "else:\n",
    "    print(\"Model exists. Proceeding with Clarify job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb563fe-bd71-4053-8691-adcd1a0750ef",
   "metadata": {},
   "source": [
    "### Start the analysis (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7aa7175b-c2cb-42be-b017-b1afe01905b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.clarify:Analysis Config: {'dataset_type': 'text/csv', 'features': 'sender,subject,body', 'headers': ['sender', 'subject', 'body', 'label'], 'label': 'label', 'predictor': {'model_name': 'spam-detection-2024-10-16-0558', 'instance_type': 'ml.m5.xlarge', 'initial_instance_count': 1, 'accept_type': 'text/csv', 'content_type': 'text/csv'}, 'methods': {'report': {'name': 'report', 'title': 'Analysis Report'}, 'shap': {'use_logit': False, 'save_local_shap_values': True, 'baseline': [['larry@google.com', 'I have your data', 'Turn on your cookies.']], 'num_samples': 15, 'agg_method': 'mean_abs'}}}\n",
      "INFO:sagemaker:Creating processing-job with name Clarify-Explainability-2024-10-16-06-33-58-193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................\u001b[34msagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\u001b[0m\n",
      "\u001b[34msagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\u001b[0m\n",
      "\u001b[34mWARNING:root:logging.conf not found when configuring logging, using default logging configuration.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Starting SageMaker Clarify Processing job\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Analysis config path: /opt/ml/processing/input/config/analysis_config.json\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Analysis result path: /opt/ml/processing/output\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:This host is algo-1.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:This host is the leader.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Number of hosts in the cluster is 1.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Running Python / Pandas based analyzer.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_factory:Dataset type: text/csv uri: /opt/ml/processing/input/data\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Loading dataset...\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Loaded dataset. Dataset info:\u001b[0m\n",
      "\u001b[34m<class 'pandas.core.frame.DataFrame'>\u001b[0m\n",
      "\u001b[34mRangeIndex: 31323 entries, 0 to 31322\u001b[0m\n",
      "\u001b[34mData columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \u001b[0m\n",
      "\u001b[34m---  ------   --------------  ----- \n",
      " 0   sender   31323 non-null  object\n",
      " 1   subject  31301 non-null  object\n",
      " 2   body     31323 non-null  object\u001b[0m\n",
      "\u001b[34mdtypes: object(3)\u001b[0m\n",
      "\u001b[34mmemory usage: 734.3+ KB\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor.managed_endpoint:Spinning up shadow endpoint\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating endpoint-config with name sm-clarify-config-1729060624-c161\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor.managed_endpoint:Creating endpoint: 'sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647'\u001b[0m\n",
      "\u001b[34mINFO:botocore.client:No endpoints ruleset found for service sagemaker-internal, falling back to legacy endpoint routing.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Using endpoint name: sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Waiting for endpoint ...\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor.managed_endpoint:Checking endpoint status:\u001b[0m\n",
      "\u001b[34mLegend:\u001b[0m\n",
      "\u001b[34m(OutOfService: x, Creating: -, Updating: -, InService: !, RollingBack: <, Deleting: o, Failed: *)\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor.managed_endpoint:Endpoint is in service after 241 seconds\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Endpoint ready.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.inferencing.batching:Single record request failed for too many times\u001b[0m\n",
      "\u001b[34mINFO:analyzer.utils.system_util:exit_message: Customer Error: An error occurred (ModelError) when calling the InvokeEndpoint operation (reached max retries: 0): Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\u001b[0m\n",
      "\u001b[34m<title>500 Internal Server Error</title>\u001b[0m\n",
      "\u001b[34m<h1>Internal Server Error</h1>\u001b[0m\n",
      "\u001b[34m<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\u001b[0m\n",
      "\u001b[34m\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647 in account 019877554860 for more information.\u001b[0m\n",
      "\u001b[34mERROR:analyzer.utils.system_util:Errors occurred when analyzing your data. Please check CloudWatch logs for more details.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/analysis_driver.py\", line 741, in main\n",
      "    violations = run_analysis(configs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/analysis_driver.py\", line 661, in run_analysis\n",
      "    metrics_results: MetricResults = run_pandas_analysis(configs)  # type: ignore\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/analysis_driver.py\", line 367, in run_pandas_analysis\n",
      "    shap_analyzer = ShapAnalyzer(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/shap/shap_analyzer.py\", line 101, in __init__\n",
      "    self.explainer = KernelExplainer(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/explainers/shap/kernel_shap.py\", line 410, in __init__\n",
      "    model_out = self.model(self.bg_dataset) if bg_dataset_model_out is None else bg_dataset_model_out\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/shap/shap_analyzer.py\", line 78, in __predict\n",
      "    pred = self.predictor.predict_proba(data)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/predictor/predictor.py\", line 343, in predict_proba\n",
      "    predicted_labels = self.__predict(data, self.__extract_predicted_score)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/predictor/predictor.py\", line 462, in __predict\n",
      "    self.__handle_errors(records_count=records_count, e=e)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/predictor/predictor.py\", line 555, in __handle_errors\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/predictor/predictor.py\", line 446, in __predict\n",
      "    prediction = self.__do_predict(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/analyzer/predictor/predictor.py\", line 573, in __do_predict\n",
      "    inference = self.predictor.predict(data, self._initial_args, self._target_model)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sagemaker/base_predictor.py\", line 212, in predict\n",
      "    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/botocore/client.py\", line 553, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/botocore/client.py\", line 1009, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\u001b[0m\n",
      "\u001b[34mbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation (reached max retries: 0): Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\u001b[0m\n",
      "\u001b[34m<title>500 Internal Server Error</title>\u001b[0m\n",
      "\u001b[34m<h1>Internal Server Error</h1>\u001b[0m\n",
      "\u001b[34m<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\u001b[0m\n",
      "\u001b[34m\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647 in account 019877554860 for more information.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Deleting endpoint configuration with name: sm-clarify-config-1729060624-c161\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Deleting endpoint with name: sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor.managed_endpoint:Model endpoint delivered 0.30438 requests per second and a total of 2 requests over 7 seconds\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor.predictor:Stop using endpoint: None\u001b[0m\n",
      "\u001b[34m---!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job Clarify-Explainability-2024-10-16-06-33-58-193: Failed. Reason: ClientError: An error occurred (ModelError) when calling the InvokeEndpoint operation (reached max retries: 0): Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647 in account 019877554860 for more information., exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclarify_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_explainability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainability_data_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshap_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/clarify.py:2366\u001b[0m, in \u001b[0;36mSageMakerClarifyProcessor.run_explainability\u001b[0;34m(self, data_config, model_config, explainability_config, model_scores, wait, logs, job_name, kms_key, experiment_config)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;66;03m# when name is either not provided (is None) or an empty string (\"\")\u001b[39;00m\n\u001b[1;32m   2363\u001b[0m job_name \u001b[38;5;241m=\u001b[39m job_name \u001b[38;5;129;01mor\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mname_from_base(\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name_prefix \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClarify-Explainability\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2365\u001b[0m )\n\u001b[0;32m-> 2366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m    \u001b[49m\u001b[43manalysis_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/clarify.py:2025\u001b[0m, in \u001b[0;36mSageMakerClarifyProcessor._run\u001b[0;34m(self, data_config, analysis_config, wait, logs, job_name, kms_key, experiment_config)\u001b[0m\n\u001b[1;32m   2009\u001b[0m data_input \u001b[38;5;241m=\u001b[39m ProcessingInput(\n\u001b[1;32m   2010\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2011\u001b[0m     source\u001b[38;5;241m=\u001b[39mdata_config\u001b[38;5;241m.\u001b[39ms3_data_input_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     s3_compression_type\u001b[38;5;241m=\u001b[39mdata_config\u001b[38;5;241m.\u001b[39ms3_compression_type,\n\u001b[1;32m   2017\u001b[0m )\n\u001b[1;32m   2018\u001b[0m result_output \u001b[38;5;241m=\u001b[39m ProcessingOutput(\n\u001b[1;32m   2019\u001b[0m     source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_CLARIFY_OUTPUT,\n\u001b[1;32m   2020\u001b[0m     destination\u001b[38;5;241m=\u001b[39mdata_config\u001b[38;5;241m.\u001b[39ms3_output_path,\n\u001b[1;32m   2021\u001b[0m     output_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_result\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2022\u001b[0m     s3_upload_mode\u001b[38;5;241m=\u001b[39mProcessingOutputHandler\u001b[38;5;241m.\u001b[39mget_s3_upload_mode(analysis_config),\n\u001b[1;32m   2023\u001b[0m )\n\u001b[0;32m-> 2025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdata_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mresult_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/processing.py:277\u001b[0m, in \u001b[0;36mProcessor.run\u001b[0;34m(self, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/processing.py:1113\u001b[0m, in \u001b[0;36mProcessingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for the processing job to complete.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m    logs (bool): Whether to show the logs produced by the job (default: True).\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m-> 1113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_processing_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_processing_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5929\u001b[0m, in \u001b[0;36mSession.logs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   5926\u001b[0m             state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   5928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 5929\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   5931\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job Clarify-Explainability-2024-10-16-06-33-58-193: Failed. Reason: ClientError: An error occurred (ModelError) when calling the InvokeEndpoint operation (reached max retries: 0): Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/sm-clarify-spam-detection-2024-10-16-0558-1729060624-c647 in account 019877554860 for more information., exit code: 1"
     ]
    }
   ],
   "source": [
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323b4af-5d37-4230-bcbf-89a5213ea5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your S3 bucket and prefix\n",
    "s3_bucket = bucket\n",
    "s3_prefix = \"clarify_output/\"\n",
    "# Ensure it ends with '/'\n",
    "\n",
    "# List SHAP output JSON files\n",
    "response = s3.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "\n",
    "# Check if objects exist\n",
    "#if 'Contents' not in response:\n",
    "#    raise ValueError(f\"No objects found with prefix '{s3_prefix}' in bucket '{s3_bucket}'.\")\n",
    "\n",
    "# Extract JSON file keys\n",
    "response = s3.list_objects_v2(Bucket=s3_bucket, Prefix='empty_prefix/')\n",
    "\n",
    "if 'Contents' in response:\n",
    "    file_keys = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.json')]\n",
    "    print(file_keys)\n",
    "else:\n",
    "    print(\"No objects found with the specified prefix.\")\n",
    "\n",
    "#if not file_keys:\n",
    "#    raise ValueError(f\"No JSON files found with prefix '{s3_prefix}' in bucket '{s3_bucket}'.\")\n",
    "\n",
    "print(\"SHAP output files:\", file_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2804f48-dc12-489a-90e4-d1f4b137869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Download SHAP JSON files\n",
    "local_shap_dir = 'shap_values'\n",
    "os.makedirs(local_shap_dir, exist_ok=True)\n",
    "\n",
    "for key in file_keys:\n",
    "    filename = os.path.basename(key)\n",
    "    local_path = os.path.join(local_shap_dir, filename)\n",
    "    s3.download_file(s3_bucket, key, local_path)\n",
    "    print(f\"Downloaded {key} to {local_path}\")\n",
    "\n",
    "# Load SHAP values from JSON files\n",
    "shap_values_list = []\n",
    "\n",
    "json_files = glob.glob(os.path.join(local_shap_dir, '*.json'))\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for record in data:\n",
    "            shap_values_list.append(record['shap_values'])  # Adjust based on actual JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe38ab7-98b8-410c-bd54-22f8bb12335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "shap_df = pd.DataFrame(shap_values_list)\n",
    "shap_df['instance_id'] = instance_ids\n",
    "shap_df.set_index('instance_id', inplace=True)\n",
    "\n",
    "# Load your training data (ensure it aligns with SHAP values)\n",
    "train_data = pd.read_csv(\"train/train.csv\")  # Replace with your actual data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae5e10-55e6-4a2b-99ed-f8cccd55b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the number of instances matches\n",
    "if len(shap_df) != len(train_data):\n",
    "    raise ValueError(\"The number of SHAP values does not match the number of instances in the training data.\")\n",
    "\n",
    "print(\"Number of SHAP values matches the number of training data instances.\")\n",
    "\n",
    "# Convert SHAP values to NumPy array\n",
    "shap_values_array = shap_df.values\n",
    "\n",
    "print(shap_values_array.shape)  # Should be [num_instances, num_features]\n",
    "\n",
    "# Visualize Summary Plot\n",
    "shap.summary_plot(shap_values_array, train_data)\n",
    "\n",
    "# Visualize Force Plot for the first instance\n",
    "shap.initjs()\n",
    "instance_index = 0\n",
    "shap.force_plot(\n",
    "    base_value=None,\n",
    "    shap_values=shap_values_array[instance_index],\n",
    "    features=train_data.iloc[instance_index, :],\n",
    "    feature_names=train_data.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b9919-afac-4e1c-b07f-6669d6173afb",
   "metadata": {},
   "source": [
    "### Going with AWS example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164926b-2e92-4955-ad66-3a6746f687fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results_uri = f\"{s3_key}/baselining\"\n",
    "validation_dataset = \"validation_dataset.csv\"\n",
    "label = train_data[\"label\"]\n",
    "\n",
    "model_explainability_baselining_job_result_uri = f\"{baseline_results_uri}/model_explainability\"\n",
    "model_explainability_data_config = DataConfig(\n",
    "    s3_data_input_path=validation_dataset,\n",
    "    s3_output_path=model_explainability_baselining_job_result_uri,\n",
    "    label=label,\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d884be7-bf83-47de-860e-ed423ce8cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = pd.read_csv(\"test_dataset.csv\", header=None)\n",
    "shap_baseline = [list(test_dataframe.mean())]\n",
    "\n",
    "shap_config = SHAPConfig(\n",
    "    baseline=shap_baseline,\n",
    "    num_samples=100,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906efcb-6a04-4e2c-920e-48018031fec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
